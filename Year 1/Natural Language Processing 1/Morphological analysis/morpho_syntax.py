# -*- coding: utf-8 -*-
"""morpho_syntax.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GVvoTfextYpukJT6e48CjQJN3ZUtUclW
"""

from google.colab import files, auth, drive

# Mount to drive
drive.mount('/content/gdrive', force_remount=True)
data_dir_drive ='/content/gdrive/My Drive/Colab Notebooks/NLP1/Lab3'

"""# 1
Choose a wikipedia article. You will download and acces the article using this python module: wikipedia. Use the content property to extract the text. Print the title of the article and the first N=200 words from the article to verify that all works well. Print the POS-tagging for the first N=20 sentences.
"""

! pip install wikipedia

! pip install nltk

import nltk
nltk.download()

import wikipedia
from nltk.tokenize import sent_tokenize, word_tokenize

wikipedia.set_lang("en")
palde = wikipedia.page("George Emil Palade")
print(palde.title)
data = palde.content
words = word_tokenize(data)
words2 = [x for x in words]
print(words2[:200])

N=20

sentences = sent_tokenize(data)
for i in range(N):
    w_ = word_tokenize(sentences[i])
    w_tags = nltk.pos_tag(w_)
    print('Sent ' + str(i + 1) + ':')
    print(w_tags)

"""# 2 
Create a function that receives a part of speech tag and returns a list with all the words from the text (can be given as a parameter too) that represent that part of speech. Create a function that receives a list of POS tags and returns a list with words having any of the given POS tags (use the first function in implementing the second one).
"""

def get_all_words_that_have_part_of_speech(partOfSpeech, data = data):
    sentences = sent_tokenize(data)
    res = []
    for s in sentences:
        w_ = word_tokenize(s)
        w_tags = nltk.pos_tag(w_)
        for (w, tag) in w_tags:
            if tag == partOfSpeech:
                res.append(w)

    return res

print(get_all_words_that_have_part_of_speech('CD'))

def get_all_words_that_have_part_of_speech_all(partsOfSpeech, data = data):
    res = []
    for tag in partsOfSpeech:
        res = res + get_all_words_that_have_part_of_speech(tag, data)
    return list(set(res))

print(get_all_words_that_have_part_of_speech_all(['JJ', 'NN']))

"""# 3 
Use the function above to print all the nouns (there are multiple tags for nouns), and, respectively all the verbs (corresponding to all verb tags). Also, print the percentage of content words (noun+verbs) from the entire text
"""

nouns = get_all_words_that_have_part_of_speech_all(['NN', 'NNS', 'NNP', 'NNPS'])
verbs = get_all_words_that_have_part_of_speech_all(['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'])

print(nouns)
print(verbs)
print(str((len(nouns) + len(verbs)) *100 / len(word_tokenize(data))) + '%')

"""# 4 
Print a table of four columns. The columns will be separated with the character "|". The head of the table will be:
Original word | POS | Simple lemmatization | Lemmatization
 with POS
that will compare the results of lemmatization (WordNetLemmatizer) without giving the part of speech and the lemmatization with the given part of speech for each word. The table must contain only words that give different results for the two lemmatizations (for example, the word "running" - without POS, the result will always be running, but with pos="v" it is "run"). The table will contain the results for the first N sentences from the text (each row corresponding to a word). Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row).
"""

! pip install prettytable

import prettytable
from nltk.stem import WordNetLemmatizer

from nltk.corpus import wordnet as wn

# WordNet POS tags are: NOUN = 'n', ADJ = 's', VERB = 'v', ADV = 'r', ADJ_SAT = 'a'
# Descriptions (c) https://web.stanford.edu/~jurafsky/slp3/10.pdf
tag_map = {
        'CC':None, # coordin. conjunction (and, but, or)  
        'CD':wn.NOUN, # cardinal number (one, two)             
        'DT':None, # determiner (a, the)                    
        'EX':wn.ADV, # existential ‘there’ (there)           
        'FW':None, # foreign word (mea culpa)             
        'IN':wn.ADV, # preposition/sub-conj (of, in, by)   
        'JJ':wn.ADJ, # adjective (yellow)                  
        'JJR':wn.ADJ, # adj., comparative (bigger)          
        'JJS':wn.ADJ, # adj., superlative (wildest)           
        'LS':None, # list item marker (1, 2, One)          
        'MD':None, # modal (can, should)                    
        'NN':wn.NOUN, # noun, sing. or mass (llama)          
        'NNS':wn.NOUN, # noun, plural (llamas)                  
        'NNP':wn.NOUN, # proper noun, sing. (IBM)              
        'NNPS':wn.NOUN, # proper noun, plural (Carolinas)
        'PDT':[wn.ADJ, wn.ADJ_SAT], # predeterminer (all, both)            
        'POS':None, # possessive ending (’s )               
        'PRP':None, # personal pronoun (I, you, he)     
        'PRP$':None, # possessive pronoun (your, one’s)    
        'RB':wn.ADV, # adverb (quickly, never)            
        'RBR':wn.ADV, # adverb, comparative (faster)        
        'RBS':wn.ADV, # adverb, superlative (fastest)     
        'RP':wn.ADJ, # particle (up, off)
        'SYM':None, # symbol (+,%, &)
        'TO':None, # “to” (to)
        'UH':None, # interjection (ah, oops)
        'VB':wn.VERB, # verb base form (eat)
        'VBD':wn.VERB, # verb past tense (ate)
        'VBG':wn.VERB, # verb gerund (eating)
        'VBN':wn.VERB, # verb past participle (eaten)
        'VBP':wn.VERB, # verb non-3sg pres (eat)
        'VBZ':wn.VERB, # verb 3sg pres (eats)
        'WDT':None, # wh-determiner (which, that)
        'WP':None, # wh-pronoun (what, who)
        'WP$':None, # possessive (wh- whose)
        'WRB':None, # wh-adverb (how, where)
        '$':None, #  dollar sign ($)
        '#':None, # pound sign (#)
        '“':None, # left quote (‘ or “)
        '”':None, # right quote (’ or ”)
        '(':None, # left parenthesis ([, (, {, <)
        ')':None, # right parenthesis (], ), }, >)
        ',':None, # comma (,)
        '.':None, # sentence-final punc (. ! ?)
        ':':None # mid-sentence punc (: ; ... – -)
    }

t = prettytable.PrettyTable(["Original word", "POS", "Simple lemmatization", "Lemmatization with POS"])

lem=WordNetLemmatizer()

N = 5

sentences = sent_tokenize(data)

word_and_part_of_speech = []

for i in range(N):
    w_ = word_tokenize(sentences[i])
    w_tags = nltk.pos_tag(w_)
    word_and_part_of_speech = word_and_part_of_speech + w_tags

word_and_part_of_speech = list(set(word_and_part_of_speech))

print(word_and_part_of_speech)

for word, tag in word_and_part_of_speech:
    lemW = lem.lemmatize(word)
    if tag in tag_map.keys():
        if tag_map[tag] is not None:
            lemWtag = lem.lemmatize(word, tag_map[tag])

            if lemW != lemWtag:
                t.add_row([word, tag, lemW, lemWtag]) 
print(t)

"""# 5 
Print a graphic showing the number of words for each part of speech. If there are too many different parts of speech, you can print only those with a higher number of corresponding words.
"""

from nltk.data import load
tagdict = load('help/tagsets/upenn_tagset.pickle')

nums = []

for tag in tagdict.keys():
    nums.append(len(get_all_words_that_have_part_of_speech(tag)))

import matplotlib.pyplot as plt; plt.rcdefaults()
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure
figure(num=None, figsize=(10, 15), dpi=80, facecolor='w', edgecolor='k')

objects = tagdict.keys()
y_pos = np.arange(len(objects))

plt.barh(y_pos, nums, align='center', alpha=0.5)
plt.yticks(y_pos, objects)
plt.xlabel('Num of words')
plt.ylabel('POS tag')
plt.title('Words per part of speech')

plt.show()