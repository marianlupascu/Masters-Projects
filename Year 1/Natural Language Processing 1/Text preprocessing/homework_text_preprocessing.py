# -*- coding: utf-8 -*-
"""homework_text_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a_Se31EOsjm6_aRTOcl-G6sNWWig3FS9
"""

from google.colab import files, auth, drive

# Mount to drive
drive.mount('/content/gdrive', force_remount=True)
data_dir_drive ='/content/gdrive/My Drive/Colab Notebooks/NLP1/Tema2'

"""# 1
Choose a wikipedia article. You will download and acces the article using this python module: wikipedia. Use the content property to extract the text. Print the title of the article and the first N=200 words from the article (use the tokenizer).
"""

! pip install wikipedia

! pip install nltk

import nltk
nltk.download()

import wikipedia
from nltk.tokenize import sent_tokenize, word_tokenize

wikipedia.set_lang("en")
palde = wikipedia.page("George Emil Palade")
print(palde.title)
data = palde.content
words = word_tokenize(data)
words2 = [x for x in words if x.isalnum()]
print(words2[:200])

"""# 2
Expand language contractions (for example change "we're" in "we are") using pycontractions
"""

! sudo apt install openjdk-8-jdk
! sudo update-alternatives --set java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java
! pip install language-check
! pip install pycontractions

! pip install pycontractions

from pycontractions import Contractions

cont = Contractions(api_key="glove-twitter-100")

data2 = list(cont.expand_texts([data]))

print(data2)

"""# 3
Create a list of all the words (in lower case), from the text, using the word tokenizer. We will name the list l_words. Remove the punctuation.
"""

dataWithContractions = data2[0]
words = word_tokenize(dataWithContractions)

l_words = [x.lower() for x in words if x.isalnum()]
# l_words = set(l_words)

print(l_words)

"""# 4
Remove stopwords from l_words and print the number of words in the text, before and after removing stopwords.
"""

print(len(l_words))

from nltk.corpus import stopwords

stopwords = stopwords.words('english')

lws = [x for x in l_words if x not in stopwords]

print(len(lws))

"""# 5 
Using RegexpTokenizer (nltk) please obtain the list of words written with capital letter that don't appear in the beginning of the phrase (name this list lnm).
"""

data

from nltk.tokenize import RegexpTokenizer 
    
# Create a reference variable for Class RegexpTokenizer 
tk1 = RegexpTokenizer('[.?!][ ]+[A-Z]\w* ')
tk2 = RegexpTokenizer('[A-Z]\w*')
    
# Use tokenize method 
list1 = tk1.tokenize(data) 
list1 = [x[2:] for x in list1]
list2 = tk2.tokenize(data)[1:]

lnm = [item for item in list2 if item not in list1]
    
print(lnm)

"""# 6
Remove all entity names (words that appear with capital letter inside the phrase, not at the beginning from l_words).
"""

l_words = [item for item in l_words if item not in lnm]

print(l_words)

"""# 7 
Apply a stemmer (Snowball) on each of the word from the list of words, l_words (without changing the list) and print the result.
"""

! pip install prettytable

import prettytable
t = prettytable.PrettyTable(["Word", "Snowball"])

snb=nltk.SnowballStemmer("english")

for x in l_words:
    snbW = snb.stem(x)
    t.add_row([x, snbW]) 
    
print(t)

"""# 8
Apply a lematizer on each of the word from the list of words, l_words (without changing the list) and print the result. Explain the different result (from the stemming process) in a comment.
"""

from nltk.stem import WordNetLemmatizer

t = prettytable.PrettyTable(["Word", "Snowball", "WordNetLemmatizer"])

snb=nltk.SnowballStemmer("english")
lem=WordNetLemmatizer()

for x in lws:
    snbW = snb.stem(x)
    lemW = lem.lemmatize(x, pos="v")

    if lemW != snbW:
        t.add_row([x, snbW, lemW]) 
print(t)

# The differences between stemming and lemmatizer processes are that: 
# in stemming, the root of a word is returned and in lemmatizer the 
# canonical form of the word is returned (as it appears in the dictionary)

"""#9 
Change all the numbers from l_words into words, using num2words. Print the number of changes, and also the portion of list that contains first N changes (for example N=10).
"""

! pip install num2words

from num2words import num2words

N = 10
count = 0

for i, x in enumerate(lws):
    if (x.isdigit()):
        lws[i] = num2words(x)
        count += 1
        if count == N:
            print(lws[:i])

print('Number of changes = ' + str(count))

"""#10
Create a function that receives a string W as parameter. The function must return a list with all the sentences containing any inflexion of the word W. For example for W=running, these sentences could be in the list: "I like to run", "He runs the fastest". You need to apply stemming both on the word W but also on each word of the sentence.
"""

def make_dict(data = data):
    sent = sent_tokenize(data)
    ps=nltk.PorterStemmer()
    dict_ = {}
    for sentence in sent:
        dict_[sentence] = [ps.stem(x.lower()) for x in word_tokenize(sentence) if x.isalnum()]
    return dict_


def get_all_sentences_with_inflections(W, data = data):
    ps=nltk.PorterStemmer()

    w = ps.stem(W.lower())

    dict_ = make_dict(data)

    list_ = []

    for key, value in dict_.items():
        if w in value:
            list_.append(key)

    return list_

res = get_all_sentences_with_inflections('Nobel')

print(res)