# -*- coding: utf-8 -*-
"""text_preprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dasfdldd30-ABiemeVAY9-Yt9S52efrL
"""

from google.colab import files, auth, drive

# Mount to drive
drive.mount('/content/gdrive', force_remount=True)
data_dir_drive ='/content/gdrive/My Drive/Colab Notebooks/NLP1/Lab2'

! pip install urllib3

"""# 1
Download it through python (inside the code, so you don't have to upload the file too when you send the solution for this exercise) with urlopen() from module urllib and read the entire text in one single string. If the download takes too much time at each running, download the file, but leave the former instructions in a comment (to show that you know how to access an online file)
"""

import urllib3
import os.path
from os import path

url = "http://www.gutenberg.org/cache/epub/32683/pg32683.txt"
http = urllib3.PoolManager()
r = http.request('GET', url, preload_content=False)

if not path.exists('data.txt'):
    open(data_dir_drive + '/data.txt', 'a').close()

with open(data_dir_drive + '/data.txt', 'wb') as out:
    while True:
        data = r.read()
        if not data:
            break
        out.write(data)
        print(data)

r.release_conn()

"""# 2
Remove the header (keep only the text starting from the title)
"""

f = open(data_dir_drive + '/data.txt', 'r')
data = f.read()

poz = data.find('The Next Time We Die')

data = data[poz:]
print(data)

"""# 3
Print the number of sentences in the text. Print the average length (number of words) of a sentence.
"""

! pip install nltk

import nltk
nltk.download()

from nltk.tokenize import sent_tokenize, word_tokenize
import numpy as np

sent = sent_tokenize(data)
words = word_tokenize(data)
print('Number of sentences in the text = ' + str(len(sent)))
print('Average length (number of words) of a sentence = ' + str(len(words) / len(sent)))

"""# 4
Create a list of all the words (in lower case) from the text, without the punctuation.
"""

words2 = [x.lower() for x in words if x.isalnum()]
words2 = set(words2)

print(words2)

"""# 5
Remove stopwords and assign the result to variable lws
"""

from nltk.corpus import stopwords

stopwords = stopwords.words('english')

lws = [x for x in words2 if x not in stopwords]

print(lws)

"""# 6
Apply stemming (Porter) on the list of words (lws). Print the first 200 words. Do you see any words that don't appear in the dictionary?
"""

ps=nltk.PorterStemmer()

psList = []

for x in lws:
    psList.append(ps.stem(x))

for x in lws[:200]:
    print(x + '\t\t\t' + ps.stem(x))

"""# 7
Print a table of three columns (of size N, where N is the maximum length for the words in the text). The columns will be separated with the character "|". The head of the table will be:
Porter    |Lancaster |Snowball
The table will contain only the words that give different stemming results for the three stemmers (for example, suppose that we have both "runs" and "being" inside the text. The word "runs" should not appear in the list, as all three results are "run"; however "being" should appear in the table). The stemming result for the word for each stemmer will appear in the table according to the head of the table. The table will contain the results for the first NW words from the text (the number of rows will obviously be less than NW, as not all words match the requirements). For example, NW=500. Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row).
"""

import prettytable
t = prettytable.PrettyTable(["Word", "Length", "Lancaster", "Snowball"])

ls=nltk.LancasterStemmer()
snb=nltk.SnowballStemmer("english")

for x in lws:
    psW = ps.stem(x)
    lsW = ls.stem(x)
    snbW = snb.stem(x)

    if psW != lsW or psW != snbW or lsW != snbW:
        t.add_row([x, psW, lsW, snbW]) 
print(t)

"""# 8
Print a table of two columns, simillar to the one above, that will compare the results of stemming and lemmatization. The head of the table will contain the values: "Snowball" and "WordNetLemmatizer". The table must contain only words that give different results in the process of stemming and lemmatization (for example, the word "running"). The table will contain the results for the first NW words from the text (the number of rows will obviously be less than NW, as not all words match the requirements). For example, NW=500. Try to print only distinct results inside the table (for example, if a word has two occurnces inside the text, and matches the requirments for appearing in the table, it should have only one corresponding row).
"""

from nltk.stem import WordNetLemmatizer

t = prettytable.PrettyTable(["Word", "Snowball", "WordNetLemmatizer"])

snb=nltk.SnowballStemmer("english")
lem=WordNetLemmatizer()

for x in lws:
    snbW = snb.stem(x)
    lemW = lem.lemmatize(x, pos="v")

    if lemW != snbW:
        t.add_row([x, snbW, lemW]) 
print(t)

"""# 9
Change all the numbers from lws into words. Print the number of changes, and also the portion of list that contains first N changes (for example N=10).
"""

! pip install num2words

from num2words import num2words

N = 10
count = 0;

for i, x in enumerate(lws):
    if (x.isdigit()):
        lws[i] = num2words(x)
        count += 1
        if count == N:
            print(lws[:i])

print('Number of changes = ' + str(count))

"""# 10
Create a function that receives an integer N and a word W as parameter (it can also receive the list of words from the text). We want to print the concordance data for that word. This means printing the window of text (words on consecutive positions) of length N, that has the givend word W in the middle. For example, for the text ""I have two dogs and a cat. Do you have pets too? My cat likes to chase mice. My dogs like to chase my cat." and a window of length 3, the concordance data for the word "cat" would be ["dogs", "cat", "pets"] and ["pets","cat", "likes"] (we consider the text without stopwords and punctuation). However, as you can see, the window of text may contain words from different sentences. Create a second function that prints windows of texts that contain words only from the phrase containing word W. We want to print concordance data for all the inflexions of word W.
"""

from nltk.corpus import stopwords

def preprocess_text(text = ''):
    words = word_tokenize(text)
    words2 = [x.lower() for x in words if x.isalnum()]
    stopwords_ = stopwords.words('english')
    lws = [x for x in words2 if x not in stopwords_]
    ps=nltk.PorterStemmer()
    psList = [ps.stem(x) for x in lws]

    for i, x in enumerate(psList):
        if (x.isdigit()):
            psList[i] = num2words(x)

    return psList


def window_of_text_of_length(words = [], N = 0, data = data):
    new_data = preprocess_text(data)
    
    ps=nltk.PorterStemmer()
    words2 = [ps.stem(x.lower()) for x in words]
    for i, x in enumerate(words2):
        if (x.isdigit()):
            words2[i] = num2words(x)

    R = []

    for word in words2:
        indices = [i for i, x in enumerate(new_data) if x == word]
        for i in indices:
            r = []
            if i - N > 1:
                r.append(new_data[i-N+1])
            r.append(new_data[i])
            if i + N < len(new_data):
                r.append(new_data[i+N-1])
            R.append(r)
            print(r)
    return R

R = window_of_text_of_length(["next", "die"], 3)
print(R)

from nltk.corpus import stopwords

def preprocess_text(text = ''):
    words = word_tokenize(text)
    words2 = [x.lower() for x in words if x.isalnum()]
    stopwords_ = stopwords.words('english')
    lws = [x for x in words2 if x not in stopwords_]
    ps=nltk.PorterStemmer()
    psList = [ps.stem(x) for x in lws]

    for i, x in enumerate(psList):
        if (x.isdigit()):
            psList[i] = num2words(x)

    return psList


def window_of_text_of_length_same_sencence(words = [], N = 0, data = data):

    sentences = sent_tokenize(data)

    R = []
    ps=nltk.PorterStemmer()
    words2 = [ps.stem(x.lower()) for x in words]
    for i, x in enumerate(words2):
        if (x.isdigit()):
            words2[i] = num2words(x)

    for sentence in sentences:

        new_data = preprocess_text(sentence)
        
        for word in words2:
            indices = [i for i, x in enumerate(new_data) if x == word]
            for i in indices:
                r = []
                if i - N > 1:
                    r.append(new_data[i-N+1])
                r.append(new_data[i])
                if i + N < len(new_data):
                    r.append(new_data[i+N-1])
                R.append(r)
                print(r)
        return R

R = window_of_text_of_length_same_sencence(["next", "die"], 3)
print(R)