{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "well_formed_substrings_table.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "haUWbgIqdmDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files, auth, drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VELUX7LyfFR7",
        "colab_type": "code",
        "outputId": "4fb2457a-c02d-4db5-caf8-3fb21a73fe9d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount to drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "data_dir_drive ='/content/gdrive/My Drive/Colab Notebooks/NLP1/Lab3'"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1L_WVClRhZt",
        "colab_type": "code",
        "outputId": "c5b84a86-4068-4f75-e7c3-3487e42671fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! pip install nltk"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1dL4MpiRxEy",
        "colab_type": "code",
        "outputId": "63fb78e7-1d2b-4dcf-b351-80e25ab96ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSApPlJHyhTf",
        "colab_type": "text"
      },
      "source": [
        "It is a type of chart parsing (in Romanian: parser cu agenda). It is used in order to avoi rebuilding subtrees that are already correct. All the above parsers used backtracking in their algorithm, therefore, we could have subtrees that are processed again because there is a wrong assumption in the subtrees created before them. It's in backtracking's nature to delete(forget) all the computing it made for a wrong prefix in the sollution. Each time we build a subtree we save it in a \"table\" and it will be reused when it's needed. An idea of implementation is given in the NLTK book (Natural Language Processing with Python, by Steven Bird, Ewan Klein, and Edward Loper, 2009).\n",
        "\n",
        "1. Supposing we have a sentence of n words, we create a matrix of (n+1)2elements. Let's call this matrix T. The meaning of this matrix is that T[i][j] wiil contan the root of the subtree containing all the words from i to j-1. At first, the matrix will be empty (initialisez with a null value), except elements T[i][i+1] that will contain the i-th word.\n",
        "2. Next we continuosly apply the productions completing the table, until no more changes in the table are made. In order to complete T[i][j] with a label, we must have a number k in [0,n] such that T[i][k] and T[k][j] are both completed (let's say T[i][k] is B and T[k][j] is C) and a production A -< B C. In this case, we'll assign T[i][j]=A. We may have multiple cases for the same line i and column j (from a different reduction of the trees). In this case, we save all the values, so it is better to consider T[i][j] being a list of symbols. an even better representantion, in order to easily obtain the responsible productions for the end tree, would be to have the whole production (A -< B C) saved in T[i][j], for example by saving it's id (assuming that all the grammar's productions have an id).\n",
        "3. How do we treat productions with a number of terminals not equal to 2. We may process the grammar and reform the productions, by adding auxiliary ones in order to obtain only two node in each production; for example \"A-> B C D\" can be changed into \"A-> B NewT1\" and \"NewT1-> C D\", where NewT1 is a new terminal used only for this production.\n",
        "4. The algorithm finishes when no more reductions can be made. If we've obtained S (the sentence node) in T[0][n] we have succesfully parsed the sentence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF5wY6lfy3pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VHOTD4VUetV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk import CFG, Nonterminal, Production\n",
        "grammar = nltk.CFG.fromstring(\"\"\"  S -> NP VP | TO VB\n",
        "                                   VP -> V NP | V NP PP | V S | V PP\n",
        "                                   PP -> P NP  \n",
        "                                   V -> \"caught\" | \"ate\" | \"likes\" | \"like\" | \"chase\" | \"go\"\n",
        "                                   NP -> Det N | Det N PP | PRP\n",
        "                                   Det -> \"the\" | \"a\" | \"an\" | \"my\" | \"some\"\n",
        "                                   N -> \"mice\" | \"cat\" | \"dog\" |  \"school\"\n",
        "                                   P -> \"in\" | \"to\" | \"on\"\n",
        "                                   TO -> \"to\"\n",
        "                                   PRP -> \"I\"  \"\"\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCnPuLxSXUUY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "outputId": "3d732c41-1f2e-470c-c28a-b98bae16bf91"
      },
      "source": [
        "#step 3\n",
        "import queue\n",
        "\n",
        "productions = grammar.productions()\n",
        "productions_ok = [] # <3 nonterminals\n",
        "productions_not_ok = queue.Queue() # >2 nonterminals\n",
        "\n",
        "for production in productions:\n",
        "    if len(production.rhs()) <= 2:\n",
        "        productions_ok.append(production)\n",
        "    else:\n",
        "        productions_not_ok.put(production)\n",
        "        print(production)\n",
        "\n",
        "print()\n",
        "index = 1\n",
        "while not productions_not_ok.empty():\n",
        "    production = productions_not_ok.get()\n",
        "    if len(production.rhs()) <= 2: #is ok\n",
        "        productions_ok.append(production)\n",
        "        print(production)\n",
        "    else:\n",
        "        # create a new productionsȘ \"A-> B C D...\" can be changed into \"A-> B NewT1\" and \"NewT1-> C D...\", \n",
        "        productions_ok.append(Production(production.lhs(), [production.rhs()[0], Nonterminal('Artificial' + str(index))])) # \"A-> B NewT1\" is ok\n",
        "        print(Production(production.lhs(), [production.rhs()[0], Nonterminal('Artificial' + str(index))]))\n",
        "        productions_not_ok.put(Production(Nonterminal('Artificial' + str(index)), production.rhs()[1:])) # \"NewT1-> C D...\" i dont know if is ok\n",
        "        index += 1\n",
        "\n",
        "print(productions_ok)\n",
        "grammar = CFG(grammar.start(), productions_ok)\n",
        "print(grammar)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "VP -> V NP PP\n",
            "NP -> Det N PP\n",
            "\n",
            "VP -> V Artificial1\n",
            "NP -> Det Artificial2\n",
            "Artificial1 -> NP PP\n",
            "Artificial2 -> N PP\n",
            "[S -> NP VP, S -> TO VB, VP -> V NP, VP -> V S, VP -> V PP, PP -> P NP, V -> 'caught', V -> 'ate', V -> 'likes', V -> 'like', V -> 'chase', V -> 'go', NP -> Det N, NP -> PRP, Det -> 'the', Det -> 'a', Det -> 'an', Det -> 'my', Det -> 'some', N -> 'mice', N -> 'cat', N -> 'dog', N -> 'school', P -> 'in', P -> 'to', P -> 'on', TO -> 'to', PRP -> 'I', VP -> V Artificial1, NP -> Det Artificial2, Artificial1 -> NP PP, Artificial2 -> N PP]\n",
            "Grammar with 32 productions (start state = S)\n",
            "    S -> NP VP\n",
            "    S -> TO VB\n",
            "    VP -> V NP\n",
            "    VP -> V S\n",
            "    VP -> V PP\n",
            "    PP -> P NP\n",
            "    V -> 'caught'\n",
            "    V -> 'ate'\n",
            "    V -> 'likes'\n",
            "    V -> 'like'\n",
            "    V -> 'chase'\n",
            "    V -> 'go'\n",
            "    NP -> Det N\n",
            "    NP -> PRP\n",
            "    Det -> 'the'\n",
            "    Det -> 'a'\n",
            "    Det -> 'an'\n",
            "    Det -> 'my'\n",
            "    Det -> 'some'\n",
            "    N -> 'mice'\n",
            "    N -> 'cat'\n",
            "    N -> 'dog'\n",
            "    N -> 'school'\n",
            "    P -> 'in'\n",
            "    P -> 'to'\n",
            "    P -> 'on'\n",
            "    TO -> 'to'\n",
            "    PRP -> 'I'\n",
            "    VP -> V Artificial1\n",
            "    NP -> Det Artificial2\n",
            "    Artificial1 -> NP PP\n",
            "    Artificial2 -> N PP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LspaudaZr8U5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "33d17543-73c7-4955-9c79-d6dde427188d"
      },
      "source": [
        "productions = grammar.productions()\n",
        "productions_ok = [] # A -> C D\n",
        "productions_not_ok = [] # A -> B\n",
        "\n",
        "for production in productions:\n",
        "    if len(production.rhs()) == 2:\n",
        "        productions_ok.append(production)\n",
        "    else:\n",
        "        if type(production.rhs()[0]) is Nonterminal:\n",
        "            productions_not_ok.append(production)\n",
        "            production_aux = grammar.productions(lhs=production.rhs()[0])\n",
        "            print(production_aux)\n",
        "            for prod_aux in production_aux:\n",
        "                productions_ok.append(Production(production.lhs(), prod_aux.rhs()))\n",
        "                print(Production(production.lhs(), prod_aux.rhs()))\n",
        "        else:\n",
        "            productions_ok.append(production)\n",
        "\n",
        "print(productions_not_ok)\n",
        "grammar = CFG(grammar.start(), productions_ok + productions_not_ok)\n",
        "print(grammar)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[PRP -> 'I']\n",
            "NP -> 'I'\n",
            "[NP -> PRP]\n",
            "Grammar with 33 productions (start state = S)\n",
            "    S -> NP VP\n",
            "    S -> TO VB\n",
            "    VP -> V NP\n",
            "    VP -> V S\n",
            "    VP -> V PP\n",
            "    PP -> P NP\n",
            "    V -> 'caught'\n",
            "    V -> 'ate'\n",
            "    V -> 'likes'\n",
            "    V -> 'like'\n",
            "    V -> 'chase'\n",
            "    V -> 'go'\n",
            "    NP -> Det N\n",
            "    NP -> 'I'\n",
            "    Det -> 'the'\n",
            "    Det -> 'a'\n",
            "    Det -> 'an'\n",
            "    Det -> 'my'\n",
            "    Det -> 'some'\n",
            "    N -> 'mice'\n",
            "    N -> 'cat'\n",
            "    N -> 'dog'\n",
            "    N -> 'school'\n",
            "    P -> 'in'\n",
            "    P -> 'to'\n",
            "    P -> 'on'\n",
            "    TO -> 'to'\n",
            "    PRP -> 'I'\n",
            "    VP -> V Artificial1\n",
            "    NP -> Det Artificial2\n",
            "    Artificial1 -> NP PP\n",
            "    Artificial2 -> N PP\n",
            "    NP -> PRP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RkrK3dwex7SG",
        "colab_type": "code",
        "outputId": "e8210cd6-07c3-472a-c722-40960b16a655",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        }
      },
      "source": [
        "#step 1 and 2 and 4\n",
        "def well_formed_substrings_table(sentence = \"\", grammar = grammar):\n",
        "    # 1\n",
        "    words = word_tokenize(sentence)\n",
        "    words2 = [x for x in words if x.isalnum()]\n",
        "    n = len(words2)\n",
        "    T = np.empty([n + 1, n + 1], dtype='object')\n",
        "    for i in range(n + 1):\n",
        "        for j in range(n + 1):\n",
        "            T[i][j] = set()\n",
        "\n",
        "    for i, word in enumerate(words2):\n",
        "        productions = grammar.productions(rhs = word)\n",
        "        for production in productions:\n",
        "            if len(production.rhs()) == 1:\n",
        "                T[i][i+1].add(production)\n",
        "\n",
        "    # 2\n",
        "    while True:\n",
        "        change = False\n",
        "        for i in range(n + 1):\n",
        "            for k in range(n + 1):\n",
        "                for j in range(i + 1, n + 1):\n",
        "                    productions1 = T[i][k]\n",
        "                    productions2 = T[k][j]\n",
        "                    for production1 in productions1:\n",
        "                        for production2 in productions2:\n",
        "                            for production in grammar.productions():\n",
        "                                if len(production.rhs()) == 2:\n",
        "                                    if production.rhs()[0] == production1.lhs() and production.rhs()[1] == production2.lhs():\n",
        "                                        if production not in T[i][j]:\n",
        "                                            T[i][j].add(production)\n",
        "                                            change = True\n",
        "        if not change:\n",
        "            break\n",
        "    print(T)\n",
        "    # 4\n",
        "    for production in T[0][n]:\n",
        "        if production.lhs() == grammar.start():\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "T = well_formed_substrings_table('I like my dog', grammar)\n",
        "print(T)\n",
        "print(grammar)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[set() {PRP -> 'I', NP -> 'I'} set() set() {S -> NP VP}]\n",
            " [set() set() {V -> 'like'} set() {VP -> V NP}]\n",
            " [set() set() set() {Det -> 'my'} {NP -> Det N}]\n",
            " [set() set() set() set() {N -> 'dog'}]\n",
            " [set() set() set() set() set()]]\n",
            "True\n",
            "Grammar with 33 productions (start state = S)\n",
            "    S -> NP VP\n",
            "    S -> TO VB\n",
            "    VP -> V NP\n",
            "    VP -> V S\n",
            "    VP -> V PP\n",
            "    PP -> P NP\n",
            "    V -> 'caught'\n",
            "    V -> 'ate'\n",
            "    V -> 'likes'\n",
            "    V -> 'like'\n",
            "    V -> 'chase'\n",
            "    V -> 'go'\n",
            "    NP -> Det N\n",
            "    NP -> 'I'\n",
            "    Det -> 'the'\n",
            "    Det -> 'a'\n",
            "    Det -> 'an'\n",
            "    Det -> 'my'\n",
            "    Det -> 'some'\n",
            "    N -> 'mice'\n",
            "    N -> 'cat'\n",
            "    N -> 'dog'\n",
            "    N -> 'school'\n",
            "    P -> 'in'\n",
            "    P -> 'to'\n",
            "    P -> 'on'\n",
            "    TO -> 'to'\n",
            "    PRP -> 'I'\n",
            "    VP -> V Artificial1\n",
            "    NP -> Det Artificial2\n",
            "    Artificial1 -> NP PP\n",
            "    Artificial2 -> N PP\n",
            "    NP -> PRP\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}