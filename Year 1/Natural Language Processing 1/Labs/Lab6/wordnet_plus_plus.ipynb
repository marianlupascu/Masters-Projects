{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wordnet_plus_plus.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "haUWbgIqdmDw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files, auth, drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VELUX7LyfFR7",
        "colab_type": "code",
        "outputId": "30fcd2e8-8024-4b97-a9a3-403fdd7485f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Mount to drive\n",
        "drive.mount('/content/gdrive', force_remount=True)\n",
        "data_dir_drive ='/content/gdrive/My Drive/Colab Notebooks/NLP1/Lab6'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1L_WVClRhZt",
        "colab_type": "code",
        "outputId": "616ce9d0-2fe1-457a-e44b-7de3ecb26f1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "! pip install nltk"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1dL4MpiRxEy",
        "colab_type": "code",
        "outputId": "111885e9-ea55-459a-8e1e-a07acb597780",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt\n",
            "    Downloading package punkt to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt.zip.\n",
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfaidXndOnt5",
        "colab_type": "text"
      },
      "source": [
        "# **Knowledge-rich WSD based on WordNet++**\n",
        "In this laboratory we present the technique developped by Simone Paolo Ponzetto and Roberto Navigli in their article \"[Knowledge-rich Word Sense Disambiguation Rivaling Supervised Systems](https://www.aclweb.org/anthology/P10-1154.pdf)\". This approach uses supplimentary relations between words, in order to compute relatedness between concepts. All the new relations are based on wikipedia, this is the reason why in this laboratory we need to use the [wikipedia module](https://wikipedia.readthedocs.io/en/latest/) (documentation: [https://wikipedia.readthedocs.io/en/latest/code.html](https://wikipedia.readthedocs.io/en/latest/code.html)). However, some of the neede relations are not implemented in the wikipedia module, therefore, we will also need to use the requests module in order to use [MediaWiki action API](https://www.mediawiki.org/wiki/API:Main_page) to wich we'll need to transmit requests.\n",
        "\n",
        "Types of relations\n",
        "\n",
        "\"Redirect to\" relations ([https://www.mediawiki.org/wiki/API:Redirects](https://www.mediawiki.org/wiki/API:Redirects))\n",
        "disambiguation pages\n",
        "internal links\n",
        "In order to use these relations we need a **mapping** between WordNet word senses and wikipedia articles. In the article, they give as an example the word \"soda\" ([https://en.wikipedia.org/wiki/Soda](https://en.wikipedia.org/wiki/Soda)). Notice that the disambiguation page redirects to this same page: [https://en.wikipedia.org/wiki/Soda_(disambiguation)](https://en.wikipedia.org/wiki/Soda). You can see that it has multiple senses illustrated in a list of pages. you can obtain the ids of those pages with a code similar to :\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fD1hkYh-QCvj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "def getWikiSensesByWordTitle(word):\n",
        "    #create a connection(session)\n",
        "    r_session = requests.Session()\n",
        "\n",
        "    #url for the MediaWiki action API\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\", #we are creating a query\n",
        "        \"titles\": word, #for the title input param    \n",
        "        \"prop\": \"redirects\", #asking for all the redirects (to the title car)\n",
        "        \"format\": \"json\" #and we want the output in a json format\n",
        "    }\n",
        "\n",
        "    #we obtain the response to the get request with the given parmeters\n",
        "    query_response = r_session.get(url=URL, params=PARAMS)\n",
        "    json_data = query_response.json()\n",
        "\n",
        "    #print(json_data)\n",
        "    wikipedia_pages = json_data[\"query\"][\"pages\"]\n",
        "\n",
        "    #we iterate through items and print all the redirects (their title and id)\n",
        "    try:\n",
        "        L = []\n",
        "        for k, v in wikipedia_pages.items():\n",
        "            for redir in v[\"redirects\"]:\n",
        "                L.append((redir[\"title\"], redir[\"pageid\"]))\n",
        "                #print(\"{} redirect to {}({})\".format(redir[\"title\"], v[\"title\"], redir[\"pageid\"]))\n",
        "        return L\n",
        "    except KeyError as err:\n",
        "        if err.args[0]=='redirects':\n",
        "            #print(\"It has no redirects\")\n",
        "            return []\n",
        "        else:\n",
        "            print(repr(err))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "laOG4nWapWGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getWikiSensesByWordId(id):\n",
        "    #create a connection(session)\n",
        "    r_session = requests.Session()\n",
        "\n",
        "    #url for the MediaWiki action API\n",
        "    URL = \"https://en.wikipedia.org/w/api.php\"\n",
        "\n",
        "    PARAMS = {\n",
        "        \"action\": \"query\", #we are creating a query\n",
        "        \"pageids\": id, #for the id input param    \n",
        "        \"prop\": \"redirects\", #asking for all the redirects (to the title car)\n",
        "        \"format\": \"json\" #and we want the output in a json format\n",
        "    }\n",
        "\n",
        "    #we obtain the response to the get request with the given parmeters\n",
        "    query_response = r_session.get(url=URL, params=PARAMS)\n",
        "    json_data = query_response.json()\n",
        "\n",
        "    #print(json_data)\n",
        "    wikipedia_pages = json_data[\"query\"][\"pages\"]\n",
        "\n",
        "    #we iterate through items and print all the redirects (their title and id)\n",
        "    try:\n",
        "        L = []\n",
        "        for k, v in wikipedia_pages.items():\n",
        "            for redir in v[\"redirects\"]:\n",
        "                L.append((redir[\"title\"], redir[\"pageid\"]))\n",
        "                #print(\"{} redirect to {}({})\".format(redir[\"title\"], v[\"title\"], redir[\"pageid\"]))\n",
        "        return L\n",
        "    except KeyError as err:\n",
        "        if err.args[0]=='redirects':\n",
        "            #print(\"It has no redirects\")\n",
        "            return []\n",
        "        else:\n",
        "            print(repr(err))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFFxZUufQIoa",
        "colab_type": "text"
      },
      "source": [
        "Notice the normalization field, it is not what you might expect; it doesn't obtain the lemma, or apply any transformation on the letter case, it is about [Unicode normalization](http://www.unicode.org/reports/tr15/).\n",
        "\n",
        "for disambiaguations, notice the following two links:\n",
        "\n",
        "[https://en.wikipedia.org/w/api.php?action=query&titles=soda&prop=pageprops&format=json](https://en.wikipedia.org/w/api.php?action=query&titles=soda&prop=pageprops&format=json)\n",
        "[https://en.wikipedia.org/w/api.php?action=query&titles=car&prop=pageprops&format=json](https://en.wikipedia.org/w/api.php?action=query&titles=car&prop=pageprops&format=json)\n",
        "In order to create the mapping we shall use for a given wikipedia page:\n",
        "\n",
        "sense labels (actually they are the titles of the pages. At the time when the article was written, the titles had this syntax \"word(sense label)\" like \"soda(soft drink)\", however, notice that now you only find the sense label as a title.\n",
        "links (outgoing links from the current page)\n",
        "categories\n",
        "The article uses the notation Ctx(w) for the set of words obtained from the text of some or all these pages.\n",
        "\n",
        "Next, we need the WordNet context for a sense s, Ctx(s), for each sense of the word. For this we take the following relations:\n",
        "\n",
        "synonymy\n",
        "hypernymy/hyponymy\n",
        "sisterhood (senses that have the same direct hypernym)\n",
        "gloss\n",
        "The next step is the mapping\n",
        "\n",
        "For each word that we want to disambiguate, if we have **only one sense**, and only one wikipedia page, we map taht wikipedia page to the word.\n",
        "In the case of multiple senses, for each remaining wikipedia word w(after the mapping from the former step) that still has no associate Wordnet word, we take all the redirects to the word w. For each such redirect we look if we already have a mapping associated to it (a relation between its sense and the wikipedia page). If we have such a mapping and the mapped word is in w's sysnset, we map w to the sense associated to the redirect page\n",
        "For all wikipedia pages that aren't mapped yet, we try to assign the most probable sense. The most probable sense has the highest value p computed as score(w,sense)/sum, where sum is the sum between all the combinations of scores between each sense of the word from wordnet and each senese of the word from wikipedia. the score is the number of common words between the context of the sense in wikipedia and the context of the sense in WordNet to which we add 1 score(s,w)=|Ctx(s) ∩ Ctx(w)|+1\n",
        "In the end we have created new relations (WordNet++) that we can use in a **simplified Lesk manner** to disambiguate a text. We will compute the overlaps on all the glosses given by the mentioned relations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNU6W7WA02Iu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word = 'soda'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k-BEkQnQkUZ",
        "colab_type": "code",
        "outputId": "f96a64d1-cf4f-4d20-c18d-9431f3197d2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from nltk.corpus import wordnet\n",
        "\n",
        "WNsenses = wordnet.synsets(word)\n",
        "print(WNsenses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('sodium_carbonate.n.01'), Synset('pop.n.02')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Aa0PYL1cban",
        "colab_type": "code",
        "outputId": "d413b5cc-3ab2-4991-8890-53bc0a4bc0ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "wikiSenses = getWikiSensesByWordTitle(word)\n",
        "print(wikiSenses)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('SODA', 3318069), ('Sodas', 5599353), ('Soday', 13752636), ('Soda (disambiguation)', 19677220), ('Soda drink', 47486780)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5srZ2jXEfXSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result = {}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oN8BJg7heQY5",
        "colab_type": "code",
        "outputId": "a23fcfac-1423-426c-a611-21dfd17d5dcc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#lines 1-2\n",
        "for w in wikiSenses:\n",
        "    result[w] = None\n",
        "\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('SODA', 3318069): None,\n",
              " ('Soda (disambiguation)', 19677220): None,\n",
              " ('Soda drink', 47486780): None,\n",
              " ('Sodas', 5599353): None,\n",
              " ('Soday', 13752636): None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MZBEW03flkR",
        "colab_type": "code",
        "outputId": "3730f00c-8227-4375-a20c-0630d69a2cef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "#lines 3-5\n",
        "for w in wikiSenses:\n",
        "    wiki = getWikiSensesByWordId(w[1])\n",
        "    wn = wordnet.synsets(w[0])\n",
        "    print(wiki, wn)\n",
        "    if (len(wiki) == 1 or len(wiki) == 0) and len(wn) == 1:\n",
        "        result[w] = wn[0]\n",
        "\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[] [Synset('sodium_carbonate.n.01'), Synset('pop.n.02')]\n",
            "[] [Synset('sodium_carbonate.n.01'), Synset('pop.n.02')]\n",
            "[] []\n",
            "[] []\n",
            "[] []\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('SODA', 3318069): None,\n",
              " ('Soda (disambiguation)', 19677220): None,\n",
              " ('Soda drink', 47486780): None,\n",
              " ('Sodas', 5599353): None,\n",
              " ('Soday', 13752636): None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9xqciMwgqAx",
        "colab_type": "code",
        "outputId": "8791af92-0f9b-4367-f2cd-a13e8cb49cab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "#lines 6-10\n",
        "for w in wikiSenses:\n",
        "    if result[w] is None:\n",
        "        wiki = getWikiSensesByWordId(w[1])\n",
        "        for d in wiki:\n",
        "            if result[d] is not None and result[w] in wordnet.synsets(w[0]):\n",
        "                result[w] = result[d]\n",
        "                break\n",
        "\n",
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('SODA', 3318069): None,\n",
              " ('Soda (disambiguation)', 19677220): None,\n",
              " ('Soda drink', 47486780): None,\n",
              " ('Sodas', 5599353): None,\n",
              " ('Soday', 13752636): None}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUZ1rojM7T98",
        "colab_type": "code",
        "outputId": "bbe9a488-0441-431b-f419-2ce8a54cff58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "! pip install wikipedia"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.9)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=1b24ea7c5c2acff6b622dd6b4346ab87aed48a86190587f3e1dccb3a9b24eb9b\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LoiDxS7K4a4g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "def getContextWikipedia(title, id):\n",
        "    wikipedia.set_lang(\"en\")\n",
        "    try:\n",
        "        page = wikipedia.page(title = title, pageid = id)\n",
        "        links = page.links\n",
        "        categs = page.categories\n",
        "    except:\n",
        "        return title\n",
        "\n",
        "    link_s = ''\n",
        "    for link in links:\n",
        "        link_s += link + ' '\n",
        "    categ_s = ''\n",
        "    for categ in categs:\n",
        "        categ_s += categ + ' '\n",
        "\n",
        "    return title + ' ' + link_s + categ_s\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwZmcNrCD62o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getSisterhood(syn):\n",
        "    L = []\n",
        "\n",
        "    for hypernym in syn.hypernyms():\n",
        "        hyponyms = hypernym.hyponyms()\n",
        "        L += hyponyms\n",
        "    return L"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDPixaL3_xBu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getContextWordNet(syn):\n",
        "    similar_to_s = ''\n",
        "    for e in syn.similar_tos():\n",
        "        similar_to_s += e.name()[:e.name().find('.')] + ' '\n",
        "\n",
        "    hypernym_s = ''\n",
        "    for hypernym in syn.hypernyms():\n",
        "        hypernym_s += hypernym.name()[:hypernym.name().find('.')] + ' '\n",
        "\n",
        "    meronym_s = ''\n",
        "    meronyms = syn.substance_meronyms() + syn.part_meronyms() + syn.part_meronyms()\n",
        "    for meronym in meronyms:\n",
        "        meronym_s += meronym.name()[:meronym.name().find('.')] + ' '\n",
        "\n",
        "    sister_s = ''\n",
        "    for sister in getSisterhood(syn):\n",
        "        sister_s += sister.name()[:sister.name().find('.')] + ' '\n",
        "\n",
        "    return syn.definition() + ' ' + similar_to_s + hypernym_s + meronym_s + sister_s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrWKt_vXH3Pw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import numpy as np\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "def compare(gloss1, gloss2):\n",
        "    words1 = word_tokenize(gloss1)\n",
        "    words2 = word_tokenize(gloss2)\n",
        "\n",
        "    words1 = [x.lower() for x in words1 if x.isalnum()]\n",
        "    words2 = [x.lower() for x in words2 if x.isalnum()]\n",
        "\n",
        "    #print(gloss1, gloss2)\n",
        "    #print(words1, words2)\n",
        "\n",
        "    ps=nltk.PorterStemmer()\n",
        "\n",
        "    stem1 = list(map(lambda x: ps.stem(x), words1))\n",
        "    stem2 = list(map(lambda x: ps.stem(x), words2))\n",
        "\n",
        "    #print(stem1, stem2)\n",
        "\n",
        "    r = 1;\n",
        "    for s1 in stem1:\n",
        "        for s2 in stem2:\n",
        "            if s1 == s2:\n",
        "                r += 1\n",
        "\n",
        "    return r"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RUf8bcUU4G3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def intersect(title, id, syn):\n",
        "\n",
        "    wiki = getContextWikipedia(title, id)\n",
        "    wn = getContextWordNet(syn)\n",
        "\n",
        "    return compare(wiki, wn)\n",
        "\n",
        "    return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S212r0lF31kC",
        "colab_type": "code",
        "outputId": "5509f3e1-28a2-46a2-aedc-281feb54512a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "#lines 11-14\n",
        "maxSum = 0\n",
        "for w in wikiSenses:\n",
        "        for syn in wordnet.synsets(word):\n",
        "            i = intersect(w[0], w[1], syn)\n",
        "            maxSum += i\n",
        "        \n",
        "for w in wikiSenses:\n",
        "    if result[w] is None:\n",
        "        max = 0\n",
        "        s = None\n",
        "        for syn in wordnet.synsets(word):\n",
        "            i = intersect(w[0], w[1], syn) / maxSum\n",
        "            #print(w,s,i)\n",
        "            if i > max:\n",
        "                max = i\n",
        "                s = syn\n",
        "        result[w] = s"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-nis-zEmKCLp",
        "colab_type": "code",
        "outputId": "0d625ecc-17bc-4703-c795-a85bdf3bd22a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "result"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{('SODA', 3318069): Synset('sodium_carbonate.n.01'),\n",
              " ('Soda (disambiguation)', 19677220): Synset('sodium_carbonate.n.01'),\n",
              " ('Soda drink', 47486780): Synset('sodium_carbonate.n.01'),\n",
              " ('Sodas', 5599353): Synset('sodium_carbonate.n.01'),\n",
              " ('Soday', 13752636): Synset('sodium_carbonate.n.01')}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MIemJs8-O_p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(data_dir_drive + '/out.txt', 'a') as the_file:\n",
        "    the_file.write(str(result))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}