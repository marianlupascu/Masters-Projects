{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fake_News_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "dc593f9df1094b5698c84fc28e1cb55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d8e56b40e57649618e1d961fd187c4fc",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_89031517d19f4efd99adbf4bbb7d3fab",
              "IPY_MODEL_54d75c2fb8854383a9f1a1c758339ce4"
            ]
          }
        },
        "d8e56b40e57649618e1d961fd187c4fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "89031517d19f4efd99adbf4bbb7d3fab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_406e33402d78454b97f655971fafa941",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 995526,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 995526,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_34308ffac4db4468be73c47807047ab9"
          }
        },
        "54d75c2fb8854383a9f1a1c758339ce4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ae29286603954a11b16fc9c21aee2586",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 996k/996k [00:00&lt;00:00, 2.10MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2ff12178d6e14056b940309052bee180"
          }
        },
        "406e33402d78454b97f655971fafa941": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "34308ffac4db4468be73c47807047ab9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ae29286603954a11b16fc9c21aee2586": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2ff12178d6e14056b940309052bee180": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e919f44f7e9145e3864f12bbc37388a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_6903f58e42a94723b673e827d81b95e0",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bcc0d60593834dd2a9b41ed868dc3115",
              "IPY_MODEL_3a12ac47ada94266b056d38f30ad070b"
            ]
          }
        },
        "6903f58e42a94723b673e827d81b95e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bcc0d60593834dd2a9b41ed868dc3115": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_e7748744d0b0474c8d8144808da3fa92",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 625,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 625,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0aec33061f1f4ced8849c9d479dfcadc"
          }
        },
        "3a12ac47ada94266b056d38f30ad070b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5de9d50097da46e78830f9d38507f1b5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 625/625 [00:10&lt;00:00, 58.1B/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_12d6e224dc1a45639d97e0b492dace7d"
          }
        },
        "e7748744d0b0474c8d8144808da3fa92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0aec33061f1f4ced8849c9d479dfcadc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5de9d50097da46e78830f9d38507f1b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12d6e224dc1a45639d97e0b492dace7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8ee0748d54534cdb9c05150cac44b48a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_a657d5f3167147539a80f86e6270ac11",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_cc5f660f879b41f0831f6df110f3458d",
              "IPY_MODEL_64192fb794054e8f9de087e896708226"
            ]
          }
        },
        "a657d5f3167147539a80f86e6270ac11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "cc5f660f879b41f0831f6df110f3458d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_81138b8ec6f145f78320f9ca05ba99b8",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 714314041,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 714314041,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f8514a7b3bae401ea3421cc309080e00"
          }
        },
        "64192fb794054e8f9de087e896708226": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3b410ee587b147ceb867b338cc784555",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 714M/714M [00:10&lt;00:00, 71.0MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e7e32981b9324069aaad5e030fe42e1c"
          }
        },
        "81138b8ec6f145f78320f9ca05ba99b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f8514a7b3bae401ea3421cc309080e00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3b410ee587b147ceb867b338cc784555": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e7e32981b9324069aaad5e030fe42e1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "660S-Mef9pox"
      },
      "source": [
        "## Imports and data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KPegK1jC3enh",
        "outputId": "ca93bdd2-d214-4748-b598-c749ae9df5e1"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0HjSXXao5eM7",
        "outputId": "8cf4c03c-ac8f-48c5-fd55-3cbf01a52d68"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "FOLDER = \"gdrive/My Drive/NLP/Fake_News\"\n",
        "dataset = pd.read_csv(FOLDER + \"/train.csv\")\n",
        "\n",
        "# Display the dataset\n",
        "print(dataset.columns)\n",
        "news = dataset['text'].apply(str)\n",
        "labels = np.array(dataset['label'])\n",
        "\n",
        "print(news)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')\n",
            "0        House Dem Aide: We Didn’t Even See Comey’s Let...\n",
            "1        Ever get the feeling your life circles the rou...\n",
            "2        Why the Truth Might Get You Fired October 29, ...\n",
            "3        Videos 15 Civilians Killed In Single US Airstr...\n",
            "4        Print \\nAn Iranian woman has been sentenced to...\n",
            "                               ...                        \n",
            "20795    Rapper T. I. unloaded on black celebrities who...\n",
            "20796    When the Green Bay Packers lost to the Washing...\n",
            "20797    The Macy’s of today grew from the union of sev...\n",
            "20798    NATO, Russia To Hold Parallel Exercises In Bal...\n",
            "20799      David Swanson is an author, activist, journa...\n",
            "Name: text, Length: 20800, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IaGHFyJWjgJf"
      },
      "source": [
        "Preprocessing the reviews"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ExYM4naAjh0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ad7f94e-ad84-4c54-9026-005d4258664d"
      },
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "\n",
        "def preprocess_data(news):\n",
        "  news = news.str.lower() # Lowercasing\n",
        "\n",
        "  # Stopwords removal\n",
        "  #nltk.download('stopwords')\n",
        "  #stop_words = stopwords.words('english')\n",
        "\n",
        "  #news = news.apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)])) # Remove stopwords\n",
        "\n",
        "  # Stemming\n",
        "  #stemmer = PorterStemmer()\n",
        "\n",
        "  #news = news.apply(lambda x: [stemmer.stem(y) for y in word_tokenize(x) if y.isalpha()])\n",
        "\n",
        "  #print(news)\n",
        "\n",
        "  #news = [' '.join(str(x) for x in article) for article in news]\n",
        "\n",
        "  print(news)\n",
        "\n",
        "  return news"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdjIzsQ6_F7V",
        "outputId": "657170b4-2110-45f0-850d-a33b8aec8c1c"
      },
      "source": [
        "news = preprocess_data(news)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        house dem aide: we didn’t even see comey’s letter until jason chaffetz tweeted it by darrell lucus on october 30, 2016 subscribe jason chaffetz on the stump in american fork, utah ( image courtesy michael jolley, available under a creative commons-by license) \\nwith apologies to keith olbermann, there is no doubt who the worst person in the world is this week–fbi director james comey. but according to a house democratic aide, it looks like we also know who the second-worst person is as well. it turns out that when comey sent his now-infamous letter announcing that the fbi was looking into emails that may be related to hillary clinton’s email server, the ranking democrats on the relevant committees didn’t hear about it from comey. they found out via a tweet from one of the republican committee chairmen. \\nas we now know, comey notified the republican chairmen and democratic ranking members of the house intelligence, judiciary, and oversight committees that his agency was reviewing emails it had recently discovered in order to see if they contained classified information. not long after this letter went out, oversight committee chairman jason chaffetz set the political world ablaze with this tweet. fbi dir just informed me, \"the fbi has learned of the existence of emails that appear to be pertinent to the investigation.\" case reopened \\n— jason chaffetz (@jasoninthehouse) october 28, 2016 \\nof course, we now know that this was not the case . comey was actually saying that it was reviewing the emails in light of “an unrelated case”–which we now know to be anthony weiner’s sexting with a teenager. but apparently such little things as facts didn’t matter to chaffetz. the utah republican had already vowed to initiate a raft of investigations if hillary wins–at least two years’ worth, and possibly an entire term’s worth of them. apparently chaffetz thought the fbi was already doing his work for him–resulting in a tweet that briefly roiled the nation before cooler heads realized it was a dud. \\nbut according to a senior house democratic aide, misreading that letter may have been the least of chaffetz’ sins. that aide told shareblue that his boss and other democrats didn’t even know about comey’s letter at the time–and only found out when they checked twitter. “democratic ranking members on the relevant committees didn’t receive comey’s letter until after the republican chairmen. in fact, the democratic ranking members didn’ receive it until after the chairman of the oversight and government reform committee, jason chaffetz, tweeted it out and made it public.” \\nso let’s see if we’ve got this right. the fbi director tells chaffetz and other gop committee chairmen about a major development in a potentially politically explosive investigation, and neither chaffetz nor his other colleagues had the courtesy to let their democratic counterparts know about it. instead, according to this aide, he made them find out about it on twitter. \\nthere has already been talk on daily kos that comey himself provided advance notice of this letter to chaffetz and other republicans, giving them time to turn on the spin machine. that may make for good theater, but there is nothing so far that even suggests this is the case. after all, there is nothing so far that suggests that comey was anything other than grossly incompetent and tone-deaf. \\nwhat it does suggest, however, is that chaffetz is acting in a way that makes dan burton and darrell issa look like models of responsibility and bipartisanship. he didn’t even have the decency to notify ranking member elijah cummings about something this explosive. if that doesn’t trample on basic standards of fairness, i don’t know what does. \\ngranted, it’s not likely that chaffetz will have to answer for this. he sits in a ridiculously republican district anchored in provo and orem; it has a cook partisan voting index of r+25, and gave mitt romney a punishing 78 percent of the vote in 2012. moreover, the republican house leadership has given its full support to chaffetz’ planned fishing expedition. but that doesn’t mean we can’t turn the hot lights on him. after all, he is a textbook example of what the house has become under republican control. and he is also the second worst person in the world. about darrell lucus \\ndarrell is a 30-something graduate of the university of north carolina who considers himself a journalist of the old school. an attempt to turn him into a member of the religious right in college only succeeded in turning him into the religious right's worst nightmare--a charismatic christian who is an unapologetic liberal. his desire to stand up for those who have been scared into silence only increased when he survived an abusive three-year marriage. you may know him on daily kos as christian dem in nc . follow him on twitter @darrelllucus or connect with him on facebook . click here to buy darrell a mello yello. connect                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
            "1        ever get the feeling your life circles the roundabout rather than heads in a straight line toward the intended destination? [hillary clinton remains the big woman on campus in leafy, liberal wellesley, massachusetts. everywhere else votes her most likely to don her inauguration dress for the remainder of her days the way miss havisham forever wore that wedding dress.  speaking of great expectations, hillary rodham overflowed with them 48 years ago when she first addressed a wellesley graduating class. the president of the college informed those gathered in 1969 that the students needed “no debate so far as i could ascertain as to who their spokesman was to be” (kind of the like the democratic primaries in 2016 minus the   terms unknown then even at a seven sisters school). “i am very glad that miss adams made it clear that what i am speaking for today is all of us —  the 400 of us,” miss rodham told her classmates. after appointing herself edger bergen to the charlie mccarthys and mortimer snerds in attendance, the    bespectacled in granny glasses (awarding her matronly wisdom —  or at least john lennon wisdom) took issue with the previous speaker. despite becoming the first   to win election to a seat in the u. s. senate since reconstruction, edward brooke came in for criticism for calling for “empathy” for the goals of protestors as he criticized tactics. though clinton in her senior thesis on saul alinsky lamented “black power demagogues” and “elitist arrogance and repressive intolerance” within the new left, similar words coming out of a republican necessitated a brief rebuttal. “trust,” rodham ironically observed in 1969, “this is one word that when i asked the class at our rehearsal what it was they wanted me to say for them, everyone came up to me and said ‘talk about trust, talk about the lack of trust both for us and the way we feel about others. talk about the trust bust.’ what can you say about it? what can you say about a feeling that permeates a generation and that perhaps is not even understood by those who are distrusted?” the “trust bust” certainly busted clinton’s 2016 plans. she certainly did not even understand that people distrusted her. after whitewater, travelgate, the vast   conspiracy, benghazi, and the missing emails, clinton found herself the distrusted voice on friday. there was a load of compromising on the road to the broadening of her political horizons. and distrust from the american people —  trump edged her 48 percent to 38 percent on the question immediately prior to november’s election —  stood as a major reason for the closing of those horizons. clinton described her vanquisher and his supporters as embracing a “lie,” a “con,” “alternative facts,” and “a   assault on truth and reason. ” she failed to explain why the american people chose his lies over her truth. “as the history majors among you here today know all too well, when people in power invent their own facts and attack those who question them, it can mark the beginning of the end of a free society,” she offered. “that is not hyperbole. ” like so many people to emerge from the 1960s, hillary clinton embarked upon a long, strange trip. from high school goldwater girl and wellesley college republican president to democratic politician, clinton drank in the times and the place that gave her a degree. more significantly, she went from idealist to cynic, as a comparison of her two wellesley commencement addresses show. way back when, she lamented that “for too long our leaders have viewed politics as the art of the possible, and the challenge now is to practice politics as the art of making what appears to be impossible possible. ” now, as the big woman on campus but the odd woman out of the white house, she wonders how her current station is even possible. “why aren’t i 50 points ahead?” she asked in september. in may she asks why she isn’t president. the woman famously dubbed a “congenital liar” by bill safire concludes that lies did her in —  theirs, mind you, not hers. getting stood up on election day, like finding yourself the jilted bride on your wedding day, inspires dangerous delusions.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
            "2        why the truth might get you fired october 29, 2016 \\nthe tension between intelligence analysts and political policymakers has always been between honest assessments and desired results, with the latter often overwhelming the former, as in the iraq war, writes lawrence davidson. \\nby lawrence davidson \\nfor those who might wonder why foreign policy makers repeatedly make bad choices, some insight might be drawn from the following analysis. the action here plays out in the united states, but the lessons are probably universal. \\nback in the early spring of 2003, george w. bush initiated the invasion of iraq. one of his key public reasons for doing so was the claim that the country’s dictator, saddam hussein, was on the verge of developing nuclear weapons and was hiding other weapons of mass destruction. the real reason went beyond that charge and included a long-range plan for “regime change” in the middle east. president george w. bush and vice president dick cheney receive an oval office briefing from cia director george tenet. also present is chief of staff andy card (on right). (white house photo) \\nfor our purposes, we will concentrate on the belief that iraq was about to become a hostile nuclear power. why did president bush and his close associates accept this scenario so readily? \\nthe short answer is bush wanted, indeed needed, to believe it as a rationale for invading iraq. at first he had tried to connect saddam hussein to the 9/11 attacks on the u.s. though he never gave up on that stratagem, the lack of evidence made it difficult to rally an american people, already fixated on afghanistan, to support a war against baghdad. \\nbut the nuclear weapons gambit proved more fruitful, not because there was any hard evidence for the charge, but because supposedly reliable witnesses, in the persons of exiled anti-saddam iraqis (many on the u.s. government’s payroll ), kept telling bush and his advisers that the nuclear story was true. \\nwhat we had was a u.s. leadership cadre whose worldview literally demanded a mortally dangerous iraq, and informants who, in order to precipitate the overthrow of saddam, were willing to tell the tale of pending atomic weapons. the strong desire to believe the tale of a nuclear iraq lowered the threshold for proof . likewise, the repeated assertions by assumed dependable iraqi sources underpinned a nationwide u.s. campaign generating both fear and war fever. \\nso the u.s. and its allies insisted that the united nations send in weapons inspectors to scour iraq for evidence of a nuclear weapons program (as well as chemical and biological weapons). that the inspectors could find no convincing evidence only frustrated the bush administration and soon forced its hand. \\non march 19, 2003, bush launched the invasion of iraq with the expectation was that, once in occupation of the country, u.s. inspectors would surely find evidence of those nukes (or at least stockpiles of chemical and biological weapons). they did not. their iraqi informants had systematically lied to them. \\nsocial and behavioral sciences to the rescue? \\nthe various u.s. intelligence agencies were thoroughly shaken by this affair, and today, 13 years later, their directors and managers are still trying to sort it out – specifically, how to tell when they are getting “true” intelligence and when they are being lied to. or, as one intelligence worker has put it, we need “ help to protect us against armies of snake oil salesmen. ” to that end the cia et al. are in the market for academic assistance. ahmed chalabi, head of the iraqi national congress, a key supplier of iraqi defectors with bogus stories of hidden wmd. \\na “partnership” is being forged between the office of the director of national intelligence (odni), which serves as the coordinating center for the sixteen independent u.s. intelligence agencies, and the national academies of sciences, engineering and medicine . the result of this collaboration will be a “ permanent intelligence community studies board” to coordinate programs in “social and behavioral science research [that] might strengthen national security .” \\ndespite this effort, it is almost certain that the “social and behavioral sciences” cannot give the spy agencies what they want – a way of detecting lies that is better than their present standard procedures of polygraph tests and interrogations. but even if they could, it might well make no difference, because the real problem is not to be found with the liars. it is to be found with the believers. \\nthe believers \\nit is simply not true, as the odni leaders seem to assert, that u.s. intelligence agency personnel cannot tell, more often than not, that they are being lied to. this is the case because there are thousands of middle-echelon intelligence workers, desk officers, and specialists who know something closely approaching the truth – that is, they know pretty well what is going on in places like afghanistan, iraq, syria, libya, israel, palestine and elsewhere. director of national intelligence james clapper (right) talks with president barack obama in the oval office, with john brennan and other national security aides present. (photo credit: office of director of national intelligence) \\ntherefore, if someone feeds them “snake oil,” they usually know it. however, having an accurate grasp of things is often to no avail because their superiors – those who got their appointments by accepting a pre-structured worldview – have different criterion for what is “true” than do the analysts. \\nlisten to charles gaukel, of the national intelligence council – yet another organization that acts as a meeting ground for the 16 intelligence agencies. referring to the search for a way to avoid getting taken in by lies, gaukel has declared, “ we’re looking for truth. but we’re particularly looking for truth that works. ” now what might that mean? \\ni can certainly tell you what it means historically. it means that for the power brokers, “truth” must match up, fit with, their worldview – their political and ideological precepts. if it does not fit, it does not “work.” so the intelligence specialists who send their usually accurate assessments up the line to the policy makers often hit a roadblock caused by “group think,” ideological blinkers, and a “we know better” attitude. \\non the other hand, as long as what you’re selling the leadership matches up with what they want to believe, you can peddle them anything: imaginary iraqi nukes, israel as a western-style democracy, saudi arabia as an indispensable ally, libya as a liberated country, bashar al-assad as the real roadblock to peace in syria, the strategic defense initiative (sdi) aka star wars, a world that is getting colder and not warmer, american exceptionalism in all its glory – the list is almost endless. \\nwhat does this sad tale tell us? if you want to spend millions of dollars on social and behavioral science research to improve the assessment and use of intelligence, forget about the liars. what you want to look for is an antidote to the narrow-mindedness of the believers – the policymakers who seem not to be able to rise above the ideological presumptions of their class – presumptions that underpin their self-confidence as they lead us all down slippery slopes. \\nit has happened this way so often, and in so many places, that it is the source of shakespeare’s determination that “what is past, is prelude.” our elites play out our destinies as if they have no free will – no capacity to break with structured ways of seeing. yet the middle-echelon specialists keep sending their relatively accurate assessments up the ladder of power. hope springs eternal.\n",
            "3        videos 15 civilians killed in single us airstrike have been identified the rate at which civilians are being killed by american airstrikes in afghanistan is now higher than it was in 2014 when the us was engaged in active combat operations.   photo of hellfire missiles being loaded onto a us military reaper drone in afghanistan by staff sgt. brian ferguson/u.s. air force. \\nthe bureau has been able to identify 15 civilians killed in a single us drone strike in afghanistan last month – the biggest loss of civilian life in one strike since the attack on the medecins sans frontieres hospital (msf) last october. \\nthe us claimed it had conducted a “counter-terrorism” strike against islamic state (is) fighters when it hit nangarhar province with missiles on september 28. but the next day the united nations issued an unusually rapid and strong statement saying the strike had killed 15 civilians and injured 13 others who had gathered at a house to celebrate a tribal elder’s return from a pilgrimage to mecca. \\nthe bureau spoke to a man named haji rais who said he was the owner of the house that was targeted. he said 15 people were killed and 19 others injured, and provided their names (listed below). the bureau was able to independently verify the identities of those who died. \\nrais’ son, a headmaster at a local school, was among them. another man, abdul hakim, lost three of his sons in the attack. \\nrais said he had no involvement with is and denied us claims that is members had visited his house before the strike. he said: “i did not even speak to those sort of people on the phone let alone receiving them in my house.” \\nthe deaths amount to the biggest confirmed loss of civilian life in a single american strike in afghanistan since the attack on the msf hospital in kunduz last october, which killed at least 42 people. \\nthe nangarhar strike was not the only us attack to kill civilians in september. the bureau’s data indicates that as many as 45 civilians and allied soldiers were killed in four american strikes in afghanistan and somalia that month. \\non september 18 a pair of strikes killed eight afghan policemen in tarinkot, the capital of urozgan provice. us jets reportedly hit a police checkpoint, killing one officer, before returning to target first responders. the use of this tactic – known as a “double-tap” strike – is controversial because they often hit civilian rescuers. \\nthe us told the bureau it had conducted the strike against individuals firing on and posing a threat to afghan forces. the email did not directly address the allegations of afghan policemen being killed. \\nat the end of the month in somalia, citizens burnt us flags on the streets of the north-central city of galcayo after it emerged a drone attack may have unintentionally killed 22 somali soldiers and civilians. the strike occurred on the same day as the one in nangarhar. \\nin both the somali and afghan incidents, the us at first denied that any non-combatants had been killed. it is now investigating both the strikes in nangarhar and galcayo. \\nthe rate at which civilians are being killed by american airstrikes in afghanistan is now higher than it was in 2014 when the us was engaged in active combat operations. name                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
            "4        print \\nan iranian woman has been sentenced to six years in prison after iran’s revolutionary guard searched her home and found a notebook that contained a fictional story she’d written about a woman who was stoned to death, according to the eurasia review . \\ngolrokh ebrahimi iraee, 35, is the wife of political prisoner arash sadeghi, 36, who is serving a 19-year prison sentence for being a human rights activist, the publication reported. \\n“when the intelligence unit of the revolutionary guards came to arrest her husband, they raided their apartment – without a warrant – and found drafts of stories that ebrahimi iraee had written,” the article stated. \\n“one of the confiscated drafts was a story about stoning women to death for adultery – never published, never presented to anyone,” the article stated. “the narrative followed the story of a protagonist that watched a movie about stoning of women under islamic law for adultery.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
            "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
            "20795    rapper t. i. unloaded on black celebrities who met with donald trump after the election, saying they failed to challenge the president for disrespecting and degrading black voters during the campaign. [the atlanta —  based artist told the   of the view thursday that he took issue with talk show host steve harvey,   kanye west, and football hall of famer and civil rights champion jim brown meeting with trump.  “before you stand and smile and say this is a good man and take pictures, what about addressing the disrespect and disregard for our community that was done?” t. i. said, adding “and what about him being the poster child for white supremacy and standing for the people who look to devalue our lives?” the   star of the vh1 reality show t. i.  tiny: the family hustle also defended fellow rapper snoop dogg, who recently starred in a   music video that sees the “doggystyle” rapper pull a gun on and “shoot” a parody clown version of president donald trump. “whatever snoop said, he had the right to say. he’s protected by the constitution in saying it,” t. i. said. “we have to protect our legends. we have to protect our heroes and the people who mean something to us and our community and our culture. ” the view   sunny hostin suggested that trump advisor omarosa manigault has stated the administration’s intention to “bridge the gap” between the white house and the black community, to which t. i. responded with a personal insult. “she can’t even bridge the gap in her teeth,” the rapper said. t. i. had originally defended snoop dogg and his controversial video in an instagram message, calling trump a “f*cking tangerine tanned muskrat scrotum skin, lacefront possum fur wig wearing, alternative fact, atomic dog diarrhea face ass man [sic]. ”   follow jerome hudson on twitter: @jeromeehudson                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
            "20796    when the green bay packers lost to the washington redskins in week 11, dropping to   aaron rodgers vowed to “run the table” in a march to the playoffs. with a   victory over the detroit lions on sunday night, the team fulfilled rodgers’ promise. much of the drama of the matchup between division rivals was eliminated earlier in the day when the redskins lost to the giants, thus guaranteeing both the packers and lions would be playoff teams, but the n. f. c. north bragging rights, and a home game in the first round of the playoffs, were sufficient motivation for green bay to push hard enough to secure the team’s sixth consecutive win and the third consecutive loss for detroit. pundits had spent the week deciphering all of the wild scenarios that could play out for positioning among the remaining teams. but when all was said and done, ten of the teams that were in line for a playoff spot remained in the same seeding order. no. 6 detroit lions at no. 3 seattle seahawks time: 8:15 p. m. eastern saturday on nbc the seahawks’ title hopes took a crushing blow when earl thomas was lost for the season with a broken leg. after the injury, the seahawks went   with the wins coming with major asterisks as they came against the   rams and 49ers. that collapse paled in comparison to the lions, who lost their final three games, blowing what had been a large division lead against green bay. line: seahawks   ( : 43) no. 5 giants at no. 4 green bay packers time: 4:40 p. m. eastern sunday on fox thanks to playing in the n. f. c. east, home of the   dallas cowboys, the giants managed to tie atlanta for the   record in the n. f. c. but got stuck with the no. 5 seed in the playoffs and a road game against a   packers squad that won its final six games. the good news for the giants is that superstitious fans will note that the last two times they played the packers on the road in the playoffs, they not only won the games but went on to win the super bowl both times. line: packers   ( : 44. 5) bye weeks: dallas, atlanta no. 5 oakland raiders at no. 4 houston texans time: 4:35 p. m. eastern saturday on espn line: texans   ( : 37) the texans were the least inspiring of the n. f. l. ’s division champions and that was complicated further when tom savage, whom the team had elevated to starting quarterback after the benching of brock osweiler, was forced to leave week 17’s loss to tennessee with a concussion. as bad as that sounds, it may still be enough against a reeling oakland squad that lost derek carr to a broken leg in week 16, matt mcgloin to a shoulder injury in week 17, and fell all the way from the no. 2 seed in the a. f. c. to no. 5. it is unclear at this point if mcgloin or rookie connor cook will start at quarterback against houston. no. 6 miami dolphins at no. 3 pittsburgh steelers time: 1:05 p. m. eastern sunday on cbs the dolphins were a contender when the team’s quarterback, ryan tannehill, was lost in week 14 with injured ligaments in his left knee. thanks to backup quarterback matt moore, and jay ajayi, the team’s   running back, they won two of three games and secured a   berth. but going up against a   offense like pittsburgh is a tough test for miami’s middling defense, even if tannehill’s knee heals enough to allow him to return. line: steelers  .5 ( : 47. 5) bye weeks: new england, kansas city with four teams vying for two n. f. c. playoff spots, all eyes were on the   game on sunday. a redskins victory could have caused movement in the seedings, with the lions and packers playing a     evening matchup. the giants, who had already locked up the no. 5 seed and had nothing to gain, threw a wrench in the redskins’ plans, eliminating their division rivals with a decisive   victory. with the drama essentially taken out of the n. f. c. all of the playoff movement sunday occurred in the a. f. c. where there was a   at the top of the standings in the a. f. c. west. just a week after losing derek carr, the team’s quarterback and a legitimate candidate for most valuable player, to a broken leg, the oakland raiders were crushed   by the denver broncos. that, combined with the kansas city chiefs’   victory over the san diego chargers vaulted the chiefs from a   spot all the way to the no. 2 seed in the a. f. c. which comes with a   bye in the playoffs. the loss for oakland added to the misery of the raiders, who have gone from super bowl contenders last week to a   team that will play on the road in houston next week potentially with a   quarterback under center as carr’s backup, matt mcgloin, injured his shoulder in the loss to the broncos. beyond the switch to the chiefs as the no.   in the a. f. c. it was business as usual for the teams that will get bye weeks in the playoffs. the new england patriots secured the no. 1 spot in the a. f. c. with a win over miami, the dallas cowboys were already guaranteed the no. 1 spot in the n. f. c. before their loss to philadelphia, and the atlanta falcons held onto the no. 2 seed in the n. f. c. with a   win over new orleans. the only remaining chance for a minor   was for the packers, who led their division by virtue of a tiebreaker, to lose to the lions, which would have forced them to play on the road in the   round of the playoffs. but the    aspect of the de facto n. f. c. north championship went away when the redskins lost to the giants, which eliminated the redskins from   contention. while most of the races were straight forward, one of the crazier playoff scenarios that had been discussed before the week was the possibility that the tampa bay buccaneers could find themselves in the playoffs. they simply had to shoot the moon by beating the carolina panthers, having the redskins tie the giants, the packers lose to the lions, and should all of that happen they simply required wins by indianapolis, dallas, tennessee and san francisco to top the packers in strength of schedule. tampa bay took care of their end by beating the panthers early in the day, but they were eliminated officially when dallas lost to philadelphia.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
            "20797    the macy’s of today grew from the union of several great names in american retailing, including its namesake chain, bloomingdale’s and marshall field’s. but the ambitious owner of saks fifth avenue has broached the idea of taking the union even further, combining with macy’s to create a department store juggernaut at a time when the industry is reeling. hudson’s bay company, the canadian owner of saks, has approached macy’s about a potential takeover, people briefed on the matter who were not authorized to speak publicly said on friday. talks between the two companies are at an early stage and may still fall apart or lead to a partnership of some kind rather than a sale. while it is unclear whether a deal will happen, a combination could lift the fortunes of macy’s, the country’s biggest department store, which has been struggling. investors certainly appeared to see it that way. shares of macy’s rose as much as 12 percent on friday, its biggest intraday gain since aug. 11, according to data from bloomberg. once a retail titan, macy’s has struggled to remain relevant as   and discount retailers have decimated the traditional    business. last month, macy’s announced plans to cut more than 10, 000 jobs and close some of its 880 stores. terry lundgren, its chief executive and the architect of macy’s last big merger, is expected to step down by the end of march. he will be succeeded by the company’s president, jeffrey gennette. since the recession, shoppers have grown accustomed to hunt for bargains and to not pay full price. discount stores and outlet malls have flourished. traditional stores have been compelled to respond by trimming prices, which cuts into their margins. departments stores have been hit especially hard, particularly as shoppers migrate away from malls. what has emerged, analysts say, is a virtual race to the bottom. that has been particularly difficult for macy’s, born of a series of mergers over the past two decades that made it a juggernaut in the industry. a stalwart of the middle tier of retail, the company has neither the advantages of   retailers like hm nor the   stores. in addition, macy’s faces increasingly fierce competition online from sites like amazon and elsewhere. macy’s troubles have drawn the attention of a prominent activist hedge fund, starboard value, which has urged the company to generate cash by selling the real estate beneath its stores. starboard, which held just under 1 percent of macy’s shares as of sept. 30, had previously estimated the value of that land at about $21 billion. on friday, analysts at citigroup estimated that macy’s   holdings could be worth at least $18 billion. macy’s market value, by comparison, was just under $11 billion as of friday morning. macy’s has taken some steps to sell or redevelop stores, and last year, it added an expert on real estate transactions to its board. but the company has largely resisted more ambitious efforts to divest its real estate, including     deals, in which a company sells the underlying land beneath its stores and then rents it back. the company’s suitor, hudson’s bay company, is far smaller  —   its market value was about 1. 9 billion canadian dollars, or $1. 5 billion  —   but is known for its bold steps. hudson’s bay company has assembled a growing empire that includes the hudson’s bay department store chain, lord  taylor and its crown jewel, saks. and the governor and executive chairman of the hudson’s bay company, richard baker, has shown little fear of using debt: in november 2014, the company borrowed nearly $4 billion against the saks flagship in midtown manhattan. he has spoken often of retailers’ need to highlight the value of their real estate. financing a bid for macy’s may be trickier, however, because the it carries about $6. 5 billion in   debt. that may mean that the hudson’s bay company will have to bring in a partner or borrow against more of its real estate holdings. a spokesman for the hudson’s bay company declined to comment on the talks, which were reported earlier by the wall street journal. “we do not comment on rumors and speculation,” a representative for macy’s said. a representative for starboard value did not respond to a request for comment. some analysts said that they saw the merit of a potential combination, particularly given macy’s operational woes and mr. baker’s expertise in wringing money out of real estate. “there is a clear logic, despite disparity in   cap” between macy’s and hudson’s bay company, craig johnson, the president of customer growth partners, a research firm, said in a note. referring to macy’s stock ticker symbol, he added, “the retail market has been changing faster than m has been able to keep up with, whether the flight from the mall or the migration online. ”                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   \n",
            "20798    nato, russia to hold parallel exercises in balkans 11/02/2016 \\npress tv \\nrussia’s military and nato forces are holding parallel military exercises in two neighboring balkan countries. \\nrussian troops will participate in war games in serbia while nato is conducting military drills in montenegro, media reported on monday. \\nrussian forces’ 13-day military exercise in serbia is named “the slavic brotherhood 2016” and begins on wednesday. \\nit will include 150 russian paratroopers, 50 air force staffers, three transport planes and an unspecified number of troops from serbia and belarus, russia’s defense ministry said. \\nthe five-day nato drill in montenegro started on monday and involves responding to floods and chemical attacks. it will involve 680 unarmed personnel from seven nato countries and 10 partner states. \\nin the past both serbia and montenegro were constitutional republics of the socialist federal republic of yugoslavia. \\nboth countries are socialist republics and traditional russian christian orthodox allies. in 2003 this state union was re-formed into serbia and montenegro, and in 2006 the two became independent states. \\nsince their split, the two balkan neighbors seem to have headed in different directions strategically. \\nmontenegro has taken a pro-western stance which has been awarded by nato with an offer to join the northern atlantic alliance. \\nthe nato invitation to montenegro has met strong opposition from russia. \\nmeanwhile, montenegrin officials have accused russia of staging an alleged coup in october to topple its pro-western government because of the nato accession bid. \\nserbia, a nato partner, has held exercises with the western alliance, but not such a large one or with foreign troops and equipment participating on its soil.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \n",
            "20799      david swanson is an author, activist, journalist, and radio host. he is a 2015 nobel peace prize nominee. he is director of worldbeyondwar.com and campaign coordinator for rootsaction.org . he hosts talk nation radio . talk nation radio is on vt radio and is syndicated by pacifica network. the show also airs on wtju, charlottesville, va; wcsx-detroit, mi; kghi, westport, wa; whus, storrs, ct; wprr, grand rapids, mi; krfp-lp, moscow, id; kzgm, cabool, mo; kmud, garberville, ca; wazu, peoria, il; wxrd, crown point, in; geneva radio, geneva, ny; kkrn, round mountain, ca; kskq-lp, ashland, or; wuow-lp, oneonta, ny; no lies radio, pinole, ca; wyap-lp, clay, wv; the detour, johnson city, tn; wzrd, chicago, il; weft, champaign, il; wxpi, pittsburgh, pa; wdrt, viroqua, wi; veracity now, online; liberty and justice radio, shirley, ma; ithaca community radio, ithaca, ny; wmcb, greenfield, ma; prx.org; kaos 89.3fm, olympia, wa; wusb 90.1 fm, stony brook, ny; wool-fm, bellow falls, vermont; wslr-lp 96.5 in sarasota, florida. he also blogs at davidswanson.org and warisacrime.org and is a prolific author. his latest books are; war is a lie , daybreak: undoing the imperial presidency and forming a more perfect union , and when the world outlawed war swanson holds a master's degree in philosophy from the university of virginia. he has worked as a newspaper reporter and as a communications director, with jobs including press secretary for dennis kucinich's 2004 presidential campaign, media coordinator for the international labor communications association, and three years as communications coordinator for acorn, the association of community organizations for reform now. read his full and complete biography at davidswanson.org and also visit book site at war is crime . what keeps the f-35 alive by david swanson on october 31, 2016 petition to stop f-35 going global \\nby david swanson \\nimagine if a local business in your town invented a brand new tool that was intended to have an almost magical effect thousands of miles away. however, where the tool was kept and used locally became an area unsafe for children. children who got near this tool tended to have increased blood pressure and increased stress hormones, lower reading skills, poorer memories, impaired auditory and speech perception, and impaired academic performance. \\nmost of us would find this situation at least a little concerning, unless the new invention was designed to murder lots of people. then it’d be just fine. \\nnow, imagine if this same new tool ruined neighborhoods because people couldn’t safely live near it. imagine if the government had to compensate people but kick them out of living near the location of this tool. again, i think, we might find that troubling if mass murder were not the mission. \\nimagine also that this tool fairly frequently explodes, emitting highly toxic chemicals, particles, and fibers unsafe to breathe into the air for miles around. normally, that’d be a problem. but if this tool is needed for killing lots of people, we’ll work with its flaws, won’t we? \\nnow, what if this new gadget was expected to cost at least $1,400,000,000,000 over 50 years? and what if that money had to be taken away from numerous other expenses more beneficial for the economy and the world? what if the $1.4 trillion was drained out of the economy causing a loss of jobs and a radical diminuition of resources for education, healthcare, housing, environmental protection, or humanitarian aid? wouldn’t that be a worry in some cases, i mean in those cases where the ability to kill tons of human beings wasn’t at stake? \\nwhat if this product, even when working perfectly, was a leading destroyer of the earth’s natural environment? \\nwhat if this high-tech toy wasn’t even designed to do what was expected of it and wasn’t even able to do what it was designed for? \\namazingly, even those shortcomings do not matter as long as the intention is massive murder and destruction. then, all is forgiven. \\nthe tool i’m describing is called the f-35. at rootsaction.org you can find a new petition launched by locally-minded people acting globally in places where the f-35 is intended to be based. also at that link you’ll find explanations of how the tool i’ve been decribing is the f-35. \\nthe petition is directed to the united states congress and the governments of australia, italy, the netherlands, norway, turkey, the united kingdom, israel, japan and south korea from the world and from the people of burlington, vermont, and fairbanks, alaska, where the f-35 is to be based. this effort is being initiated by vermont stop the f35 coalition, save our skies vermont, western maine matters, alaska peace center, university of alaska fairbanks peace club, north star chapter 146 veterans for peace, world beyond war, rootsaction.org, code pink, and ben cohen. \\nthe petition reads: \\nthe f-35 is a weapon of offensive war, serving no defensive purpose. it is planned to cost the u.s. $1.4 trillion over 50 years. because starvation on earth could be ended for $30 billion and the lack of clean drinking water for $11 billion per year, it is first and foremost through the wasting of resources that this airplane will kill. military spending, contrary to popular misconception, also hurts the u.s. economy ( see here ) and other economies. the f-35 causes negative health impacts and cognitive impairment in children living near its bases. it renders housing near airports unsuitable for residential use. it has a high crash rate and horrible consequences to those living in the area of its crashes. its emissions are a major environmental polluter. \\nwars are endangering the united states and other participating nations rather than protecting them. nonviolent tools of law, diplomacy, aid, crisis prevention, and verifiable nuclear disarmament should be substituted for continuing counterproductive wars. therefore, we, the undersigned, call for the immediate cancellation of the f-35 program as a whole, and the immediate cancellation of plans to base any such dangerous and noisy jets near populated areas. we oppose replacing the f-35 with any other weapon or basing the f-35 in any other locations. we further demand redirection of the money for the f-35 back into taxpayers’ pockets, and into environmental and human needs in the u.s., other f-35 customer nations, and around the world, including to fight climate change, pay off student debt, rebuild crumbling infrastructure, and improve education, healthcare, and housing. \\nadd your name . \\ndavid swanson is an author, activist, journalist, and radio host. he is director of worldbeyondwar.org and campaign coordinator for rootsaction.org . swanson’s books include war is a lie . he blogs at davidswanson.org and warisacrime.org . he hosts talk nation radio .he is a 2015 and 2016 nobel peace prize nominee. \\nfollow him on twitter: @davidcnswanson and facebook . \\nhelp support davidswanson.org, warisacrime.org, and talknationradio.org by clicking here: http://davidswanson.org/donate .                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
            "Name: text, Length: 20800, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A89LFGrI9uNz"
      },
      "source": [
        "## Encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gz-pTS0s7tic"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def split_dataset(text, labels):\n",
        "  X_train, X_test, Y_train, Y_test = train_test_split(text, labels, test_size = 0.1)\n",
        "  return X_train, Y_train, X_test, Y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYBZUA1w7w77"
      },
      "source": [
        "X, Y, X_test, Y_test = split_dataset(news, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNI1sgyZiTyh"
      },
      "source": [
        "### Tfidf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PY1OJ9xiwBG"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer(min_df = 5,\n",
        "                             max_df = 0.8,\n",
        "                             sublinear_tf = True,\n",
        "                             use_idf = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hccZt-fDFHiZ"
      },
      "source": [
        "X = vectorizer.fit_transform(X)\r\n",
        "X_test = vectorizer.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_FXRsvMCzuI_"
      },
      "source": [
        "### Bag of words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aStiaDcezwCI"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uw9nbtxU0IGc"
      },
      "source": [
        "X = count_vect.fit_transform(X)\n",
        "X_test = count_vect.transform(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMmlQumv94Iy"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kW6h_rgQC7cJ"
      },
      "source": [
        "### Classical approaches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSgstL1O96l9"
      },
      "source": [
        "#### SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMAumbSIkSSG",
        "outputId": "1b6618a7-3e52-404a-f37b-c2bfef51447d"
      },
      "source": [
        "from sklearn import svm\n",
        "\n",
        "classifier = svm.SVC(kernel = 'linear')\n",
        "classifier.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6F-QkAQkYQC"
      },
      "source": [
        "Get accuracy on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zd47P0MhkYlv",
        "outputId": "9084a0b5-5e8e-4d41-d367-df0a409f7285"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "prediction = classifier.predict(X_test)\n",
        "print(accuracy_score(Y_test, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9552884615384616\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsbVJTStsWAX"
      },
      "source": [
        "TFIDF:\n",
        "\n",
        "With stop words removal -> 0.9783653846153846\n",
        "\n",
        "Without stop words removal -> 0.9697115384615385\n",
        "\n",
        "Stemming without stopwords removal -> 0.9591346153846154\n",
        "\n",
        "Stemming with stopwords removal -> 0.9552884615384616\n",
        "\n",
        "BOW:\n",
        "\n",
        "With stopwords removal -> 0.9442307692307692\n",
        "\n",
        "Without stopwords removal -> 0.9552884615384616\n",
        "\n",
        "Stemming without stopwords removal -> 0.9298076923076923\n",
        "\n",
        "Stemming with stopwords removal -> 0.9278846153846154"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV88G4KXWlid"
      },
      "source": [
        "#### NB"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_gP5pgFWoFW"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\r\n",
        "classifier = MultinomialNB().fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rtb7bPASWu1M",
        "outputId": "c2bf532a-8593-4066-8d7d-43aef693a059"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\r\n",
        "\r\n",
        "prediction = classifier.predict(X_test)\r\n",
        "print(accuracy_score(Y_test, prediction))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9149038461538461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVqRigO-I8Bu"
      },
      "source": [
        "TFIDF:\n",
        "\n",
        "With stop words removal -> 0.9038461538461539\n",
        "\n",
        "Without stop words removal -> 0.9115384615384615\n",
        "\n",
        "Stemming without stopwords removal -> 0.8971153846153846\n",
        "\n",
        "Stemming with stopwords removal -> 0.8836538461538461\n",
        "\n",
        "BOW:\n",
        "\n",
        "With stopwords removal -> 0.8990384615384616\n",
        "\n",
        "Without stopwords removal -> 0.9149038461538461\n",
        "\n",
        "Stemming without stopwords removal -> 0.8682692307692308\n",
        "\n",
        "Stemming with stopwords removal -> 0.8778846153846154"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9TlD1BwYh2j"
      },
      "source": [
        "#### Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whbMXffgYj92",
        "outputId": "89eea8eb-92ee-4b69-e441-dcb582f6c827"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\r\n",
        "LR = LogisticRegression()\r\n",
        "LR.fit(X, Y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 161
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s74YS8ymYl6o",
        "outputId": "4bd725af-c539-4c1f-94d0-6601340eb6b5"
      },
      "source": [
        "print(LR.score(X_test, Y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9644230769230769\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF8IK6CzJG5k"
      },
      "source": [
        "TFIDF:\n",
        "\n",
        "With stop words removal -> 0.9634615384615385\n",
        "\n",
        "Without stop words removal -> 0.9533653846153847\n",
        "\n",
        "Stemming without stopwords removal -> 0.9418269230769231\n",
        "\n",
        "Stemming with stopwords removal -> 0.948076923076923\n",
        "\n",
        "BOW:\n",
        "\n",
        "With stopwords removal -> 0.9615384615384616\n",
        "\n",
        "Without stopwords removal -> 0.9644230769230769\n",
        "\n",
        "Stemming without stopwords removal -> 0.9394230769230769\n",
        "\n",
        "Stemming with stopwords removal -> 0.94375"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egei5e5p-CzI"
      },
      "source": [
        "### Deep learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZ7Hjdg2DArf"
      },
      "source": [
        "#### BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ha4vcGkOGoaW"
      },
      "source": [
        "X_train, Y_train, X_validation, Y_validation = split_dataset(X, Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CS-pc73EbQ8t",
        "outputId": "a5812201-acdd-4fd7-d32f-f259434abbcb"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx5jZ2QbbSTF",
        "outputId": "ad401919-bce1-4a8d-c0bc-4bb157537122"
      },
      "source": [
        "import torch\n",
        "# We will run the model on the GPU\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "dc593f9df1094b5698c84fc28e1cb55b",
            "d8e56b40e57649618e1d961fd187c4fc",
            "89031517d19f4efd99adbf4bbb7d3fab",
            "54d75c2fb8854383a9f1a1c758339ce4",
            "406e33402d78454b97f655971fafa941",
            "34308ffac4db4468be73c47807047ab9",
            "ae29286603954a11b16fc9c21aee2586",
            "2ff12178d6e14056b940309052bee180"
          ]
        },
        "id": "_VS63VDRMsxg",
        "outputId": "643a5fc8-323d-40fb-cb48-ac2665b8a15c"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case = False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dc593f9df1094b5698c84fc28e1cb55b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=995526.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GvBKbk4Muje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab3974d-a164-422b-8946-c7fb2f4b0a65"
      },
      "source": [
        "train_input_ids = []\n",
        "train_attention_masks = []\n",
        "\n",
        "for news in X_train:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        news,\n",
        "                        add_special_tokens = True, # Add [CLS] and [SEP]\n",
        "                        max_length = 128,          # Pad and truncate to 128 characters\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Create attention masks so that BERT knows not to look at padding tokens\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded value to the list\n",
        "    train_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And the attention mask\n",
        "    train_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors\n",
        "train_input_ids = torch.cat(train_input_ids, dim = 0)\n",
        "train_attention_masks = torch.cat(train_attention_masks, dim = 0)\n",
        "train_labels = torch.tensor(Y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ybSyQ-QnPGD1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "924483fd-c3b4-433b-d158-c1ea61c0a274"
      },
      "source": [
        "validation_input_ids = []\n",
        "validation_attention_masks = []\n",
        "\n",
        "for news in X_validation:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        news,\n",
        "                        add_special_tokens = True, # Add [CLS] and [SEP]\n",
        "                        max_length = 128,          # Pad and truncate to 128 characters\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Create attention masks so that BERT knows not to look at padding tokens\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded value to the list\n",
        "    validation_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And the attention mask\n",
        "    validation_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors\n",
        "validation_input_ids = torch.cat(validation_input_ids, dim = 0)\n",
        "validation_attention_masks = torch.cat(validation_attention_masks, dim = 0)\n",
        "validation_labels = torch.tensor(Y_validation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GqyrSmpAlJfT",
        "outputId": "7f24932d-0e2f-4b84-fb0e-67bacad0f74a"
      },
      "source": [
        "test_input_ids = []\n",
        "test_attention_masks = []\n",
        "\n",
        "for news in X_test:\n",
        "\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        news,\n",
        "                        add_special_tokens = True, # Add [CLS] and [SEP]\n",
        "                        max_length = 128,          # Pad and truncate to 128 characters\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Create attention masks so that BERT knows not to look at padding tokens\n",
        "                        return_tensors = 'pt',     # Return pytorch tensors\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded value to the list\n",
        "    test_input_ids.append(encoded_dict['input_ids'])\n",
        "    \n",
        "    # And the attention mask\n",
        "    test_attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "# Convert the lists into tensors\n",
        "test_input_ids = torch.cat(test_input_ids, dim = 0)\n",
        "test_attention_masks = torch.cat(test_attention_masks, dim = 0)\n",
        "test_labels = torch.tensor(Y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2143: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoRzq0zoOXpL"
      },
      "source": [
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "# Create a tensor dataset out of the ids, attention masks and labels\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_labels)\n",
        "validation_dataset = TensorDataset(validation_input_ids, validation_attention_masks, validation_labels)\n",
        "test_dataset = TensorDataset(test_input_ids, test_attention_masks, test_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUtkuQrtQh99"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create dataloaders for the training, validation and testing sets\n",
        "train_dataloader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset), # Select batches randomly\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "\n",
        "validation_dataloader = DataLoader(\n",
        "            validation_dataset,\n",
        "            sampler = SequentialSampler(validation_dataset), # The order does not matter here, so we can just read them sequentially\n",
        "            batch_size = batch_size\n",
        "        )\n",
        "test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            sampler = SequentialSampler(test_dataset), # The order does not matter here, so we can just read them sequentially\n",
        "            batch_size = batch_size\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzSthVyTQkgT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "e919f44f7e9145e3864f12bbc37388a0",
            "6903f58e42a94723b673e827d81b95e0",
            "bcc0d60593834dd2a9b41ed868dc3115",
            "3a12ac47ada94266b056d38f30ad070b",
            "e7748744d0b0474c8d8144808da3fa92",
            "0aec33061f1f4ced8849c9d479dfcadc",
            "5de9d50097da46e78830f9d38507f1b5",
            "12d6e224dc1a45639d97e0b492dace7d",
            "8ee0748d54534cdb9c05150cac44b48a",
            "a657d5f3167147539a80f86e6270ac11",
            "cc5f660f879b41f0831f6df110f3458d",
            "64192fb794054e8f9de087e896708226",
            "81138b8ec6f145f78320f9ca05ba99b8",
            "f8514a7b3bae401ea3421cc309080e00",
            "3b410ee587b147ceb867b338cc784555",
            "e7e32981b9324069aaad5e030fe42e1c"
          ]
        },
        "outputId": "6e9f2a89-c674-447f-e914-b8ed0b98b944"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW\n",
        " \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-multilingual-cased\",\n",
        "    output_attentions = False, # Ww do not need the attention weights\n",
        "    output_hidden_states = False, # We do not need to see the hidden states\n",
        ")\n",
        " \n",
        "# Run the model on the GPU\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e919f44f7e9145e3864f12bbc37388a0",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=625.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ee0748d54534cdb9c05150cac44b48a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=714314041.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax0eImtoQmp9"
      },
      "source": [
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # Learning rate\n",
        "                  eps = 1e-8 # Epsilon\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5qD50pmQm3C"
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        " \n",
        "epochs = 5\n",
        " \n",
        "# The number of training steps is [number of batches] x [number of epochs] \n",
        "total_steps = len(train_dataloader) * epochs\n",
        " \n",
        "# Use a learning rate scheduler so that we can optimize the training\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0,\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD3BRRcPQpCd"
      },
      "source": [
        "# Calculate the accuracy of our prediction vs the ground truth\n",
        "def get_accuracy(pred, labels):\n",
        "    pred_flat = np.argmax(pred, axis = 1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-axVovmQv2-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f46235d6-4b94-4ab5-8aae-a650117de934"
      },
      "source": [
        "import numpy as np\n",
        " \n",
        "training_stats = []\n",
        " \n",
        "for epoch in range(0, epochs):\n",
        "    \n",
        "    # Training\n",
        " \n",
        "    print(\"\")\n",
        "    print('Epoch {:} / {:}'.format(epoch + 1, epochs))\n",
        "    print('Training...')\n",
        " \n",
        "    total_train_loss = 0\n",
        "    model.train()\n",
        " \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        " \n",
        "        # Progress update\n",
        "        if step % 40 == 0 and not step == 0:\n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}'.format(step, len(train_dataloader)))\n",
        " \n",
        "        # Copy each tensor to the GPU\n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_input_mask = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        " \n",
        "        # Clear any previously calculated gradients\n",
        "        model.zero_grad()        \n",
        " \n",
        "        output = model(batch_input_ids, attention_mask = batch_input_mask, labels = batch_labels)\n",
        "        loss = output.loss\n",
        "        \n",
        "        # We will calculate the total training loss at the end\n",
        "        total_train_loss += loss.item()\n",
        " \n",
        "        # Calculate the gradients with a backward pass\n",
        "        loss.backward()\n",
        " \n",
        "        # Clip the norm of the gradients to 1.0 so that we can prevent the \"exploding gradients\" problem\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        " \n",
        "        # Update parameters and take a step using the computed gradient\n",
        "        optimizer.step()\n",
        " \n",
        "        # Update the learning rate\n",
        "        scheduler.step()\n",
        " \n",
        "    # Average training loss over all of the batches\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        " \n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.4f}\".format(avg_train_loss))\n",
        "        \n",
        "    # Validation\n",
        " \n",
        "    print(\"\")\n",
        "    print(\"Validation...\")\n",
        " \n",
        "    model.eval()\n",
        " \n",
        "    total_valid_accuracy = 0\n",
        "    total_valid_loss = 0\n",
        " \n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Copy each tensor to the GPU\n",
        "        batch_input_ids = batch[0].to(device)\n",
        "        batch_input_mask = batch[1].to(device)\n",
        "        batch_labels = batch[2].to(device)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            output = model(batch_input_ids, attention_mask = batch_input_mask, labels = batch_labels)\n",
        "            loss = output.loss\n",
        "            logits = output.logits # Model values before activation\n",
        "            \n",
        "        # We will calculate the total validation loss at the end\n",
        "        total_valid_loss += loss.item()\n",
        " \n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        labels = batch_labels.to('cpu').numpy()\n",
        " \n",
        "        total_valid_accuracy += get_accuracy(logits, labels)\n",
        "        \n",
        " \n",
        "    # Report the final accuracy for this validation run.\n",
        "    avg_val_accuracy = total_valid_accuracy / len(validation_dataloader)\n",
        "    print(\"  Accuracy: {0:.4f}\".format(avg_val_accuracy))\n",
        " \n",
        "    # Calculate the average validation loss over all of the batches\n",
        "    avg_val_loss = total_valid_loss / len(validation_dataloader)\n",
        "    \n",
        "    print(\"  Validation Loss: {0:.4f}\".format(avg_val_loss))\n",
        " \n",
        "    # Record all statistics from this epoch\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Validation Loss': avg_val_loss,\n",
        "            'Validation Accuracy.': avg_val_accuracy,\n",
        "        }\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 / 5\n",
            "Training...\n",
            "  Batch    40  of    527\n",
            "  Batch    80  of    527\n",
            "  Batch   120  of    527\n",
            "  Batch   160  of    527\n",
            "  Batch   200  of    527\n",
            "  Batch   240  of    527\n",
            "  Batch   280  of    527\n",
            "  Batch   320  of    527\n",
            "  Batch   360  of    527\n",
            "  Batch   400  of    527\n",
            "  Batch   440  of    527\n",
            "  Batch   480  of    527\n",
            "  Batch   520  of    527\n",
            "\n",
            "  Average training loss: 0.0999\n",
            "\n",
            "Validation...\n",
            "  Accuracy: 0.9831\n",
            "  Validation Loss: 0.0666\n",
            "\n",
            "Epoch 2 / 5\n",
            "Training...\n",
            "  Batch    40  of    527\n",
            "  Batch    80  of    527\n",
            "  Batch   120  of    527\n",
            "  Batch   160  of    527\n",
            "  Batch   200  of    527\n",
            "  Batch   240  of    527\n",
            "  Batch   280  of    527\n",
            "  Batch   320  of    527\n",
            "  Batch   360  of    527\n",
            "  Batch   400  of    527\n",
            "  Batch   440  of    527\n",
            "  Batch   480  of    527\n",
            "  Batch   520  of    527\n",
            "\n",
            "  Average training loss: 0.0303\n",
            "\n",
            "Validation...\n",
            "  Accuracy: 0.9883\n",
            "  Validation Loss: 0.0591\n",
            "\n",
            "Epoch 3 / 5\n",
            "Training...\n",
            "  Batch    40  of    527\n",
            "  Batch    80  of    527\n",
            "  Batch   120  of    527\n",
            "  Batch   160  of    527\n",
            "  Batch   200  of    527\n",
            "  Batch   240  of    527\n",
            "  Batch   280  of    527\n",
            "  Batch   320  of    527\n",
            "  Batch   360  of    527\n",
            "  Batch   400  of    527\n",
            "  Batch   440  of    527\n",
            "  Batch   480  of    527\n",
            "  Batch   520  of    527\n",
            "\n",
            "  Average training loss: 0.0096\n",
            "\n",
            "Validation...\n",
            "  Accuracy: 0.9894\n",
            "  Validation Loss: 0.0656\n",
            "\n",
            "Epoch 4 / 5\n",
            "Training...\n",
            "  Batch    40  of    527\n",
            "  Batch    80  of    527\n",
            "  Batch   120  of    527\n",
            "  Batch   160  of    527\n",
            "  Batch   200  of    527\n",
            "  Batch   240  of    527\n",
            "  Batch   280  of    527\n",
            "  Batch   320  of    527\n",
            "  Batch   360  of    527\n",
            "  Batch   400  of    527\n",
            "  Batch   440  of    527\n",
            "  Batch   480  of    527\n",
            "  Batch   520  of    527\n",
            "\n",
            "  Average training loss: 0.0031\n",
            "\n",
            "Validation...\n",
            "  Accuracy: 0.9883\n",
            "  Validation Loss: 0.0766\n",
            "\n",
            "Epoch 5 / 5\n",
            "Training...\n",
            "  Batch    40  of    527\n",
            "  Batch    80  of    527\n",
            "  Batch   120  of    527\n",
            "  Batch   160  of    527\n",
            "  Batch   200  of    527\n",
            "  Batch   240  of    527\n",
            "  Batch   280  of    527\n",
            "  Batch   320  of    527\n",
            "  Batch   360  of    527\n",
            "  Batch   400  of    527\n",
            "  Batch   440  of    527\n",
            "  Batch   480  of    527\n",
            "  Batch   520  of    527\n",
            "\n",
            "  Average training loss: 0.0003\n",
            "\n",
            "Validation...\n",
            "  Accuracy: 0.9883\n",
            "  Validation Loss: 0.0830\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKBCoYB3lvcK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "bf0edd6c-1466-4d55-ad8b-2f2e0ba42e05"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        " \n",
        "# Create a DataFrame from our training stats\n",
        "df = pd.DataFrame(data = training_stats)\n",
        " \n",
        " \n",
        "sns.set(style = 'darkgrid')\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12, 6)\n",
        " \n",
        "# Plot the training and validation losses\n",
        "plt.plot(df['Training Loss'], 'b-o', label = \"Training\")\n",
        "plt.plot(df['Validation Loss'], 'g-o', label = \"Validation\")\n",
        " \n",
        "plt.title(\"Training and Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        " \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hW2S_LimMcm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c995af9-4fb7-4ae7-bd51-bd7d0e7cb732"
      },
      "source": [
        "model.eval()\n",
        " \n",
        "predictions, ground_truth = [], []\n",
        " \n",
        "total_test_accuracy = 0\n",
        " \n",
        "for batch in test_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(i.to(device) for i in batch)\n",
        "  \n",
        "  # Get the inputs from the dataloader\n",
        "  batch_input_ids, batch_input_mask, batch_labels = batch\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      outputs = model(batch_input_ids, attention_mask = batch_input_mask)\n",
        " \n",
        "  logits = outputs.logits\n",
        " \n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  labels = batch_labels.to('cpu').numpy()\n",
        " \n",
        "  total_test_accuracy += get_accuracy(logits, labels)\n",
        "  \n",
        "  predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "  ground_truth.extend(labels.flatten())\n",
        " \n",
        "# Report the final accuracy for the test set\n",
        "avg_test_accuracy = total_test_accuracy / len(test_dataloader)\n",
        "print(\"Accuracy: {0:.4f}\".format(avg_test_accuracy))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9923\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}